{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "    \"cells\": [\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 1,\n",
    "            \"metadata\": {\n",
    "                \"execution\": {\n",
    "                    \"iopub.execute_input\": \"2021-12-09T15:42:30.890422Z\",\n",
    "                    \"iopub.status.busy\": \"2021-12-09T15:42:30.890036Z\",\n",
    "                    \"iopub.status.idle\": \"2021-12-09T15:42:37.774794Z\",\n",
    "                    \"shell.execute_reply\": \"2021-12-09T15:42:37.774062Z\",\n",
    "                    \"shell.execute_reply.started\": \"2021-12-09T15:42:30.890331Z\"\n",
    "                },\n",
    "                \"trusted\": true\n",
    "            },\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"stderr\",\n",
    "                    \"output_type\": \"stream\",\n",
    "                    \"text\": [\n",
    "                        \"2022-12-02 20:56:28.403071: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\\n\",\n",
    "                        \"To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\\n\",\n",
    "                        \"2022-12-02 20:56:30.401887: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\\n\",\n",
    "                        \"2022-12-02 20:56:30.402185: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\\n\",\n",
    "                        \"2022-12-02 20:56:30.402198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\\n\"\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"source\": [\n",
    "                \"from keras.preprocessing.text import Tokenizer\\n\",\n",
    "                \"from keras.utils import pad_sequences\\n\",\n",
    "                \"from nltk.tokenize import word_tokenize\\n\",\n",
    "                \"from nltk.stem import WordNetLemmatizer\\n\",\n",
    "                \"from nltk.corpus import stopwords\\n\",\n",
    "                \"from tensorflow.python.keras.layers import *\\n\",\n",
    "                \"from tensorflow.python.keras.models import Model\\n\",\n",
    "                \"import numpy as np \\n\",\n",
    "                \"import pandas as pd \\n\",\n",
    "                \"import re\\n\",\n",
    "                \"import nltk\\n\",\n",
    "                \"from preprocess import *\\n\",\n",
    "                \"from models import *\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 2,\n",
    "            \"metadata\": {\n",
    "                \"execution\": {\n",
    "                    \"iopub.execute_input\": \"2021-12-09T15:42:52.454527Z\",\n",
    "                    \"iopub.status.busy\": \"2021-12-09T15:42:52.453714Z\",\n",
    "                    \"iopub.status.idle\": \"2021-12-09T15:43:01.326923Z\",\n",
    "                    \"shell.execute_reply\": \"2021-12-09T15:43:01.326227Z\",\n",
    "                    \"shell.execute_reply.started\": \"2021-12-09T15:42:52.454487Z\"\n",
    "                },\n",
    "                \"trusted\": true\n",
    "            },\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"data\": {\n",
    "                        \"text/html\": [\n",
    "                            \"<div>\\n\",\n",
    "                            \"<style scoped>\\n\",\n",
    "                            \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "                            \"        vertical-align: middle;\\n\",\n",
    "                            \"    }\\n\",\n",
    "                            \"\\n\",\n",
    "                            \"    .dataframe tbody tr th {\\n\",\n",
    "                            \"        vertical-align: top;\\n\",\n",
    "                            \"    }\\n\",\n",
    "                            \"\\n\",\n",
    "                            \"    .dataframe thead th {\\n\",\n",
    "                            \"        text-align: right;\\n\",\n",
    "                            \"    }\\n\",\n",
    "                            \"</style>\\n\",\n",
    "                            \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "                            \"  <thead>\\n\",\n",
    "                            \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "                            \"      <th></th>\\n\",\n",
    "                            \"      <th>id</th>\\n\",\n",
    "                            \"      <th>qid1</th>\\n\",\n",
    "                            \"      <th>qid2</th>\\n\",\n",
    "                            \"      <th>question1</th>\\n\",\n",
    "                            \"      <th>question2</th>\\n\",\n",
    "                            \"      <th>is_duplicate</th>\\n\",\n",
    "                            \"    </tr>\\n\",\n",
    "                            \"  </thead>\\n\",\n",
    "                            \"  <tbody>\\n\",\n",
    "                            \"    <tr>\\n\",\n",
    "                            \"      <th>0</th>\\n\",\n",
    "                            \"      <td>0</td>\\n\",\n",
    "                            \"      <td>1</td>\\n\",\n",
    "                            \"      <td>2</td>\\n\",\n",
    "                            \"      <td>What is the step by step guide to invest in sh...</td>\\n\",\n",
    "                            \"      <td>What is the step by step guide to invest in sh...</td>\\n\",\n",
    "                            \"      <td>0</td>\\n\",\n",
    "                            \"    </tr>\\n\",\n",
    "                            \"    <tr>\\n\",\n",
    "                            \"      <th>1</th>\\n\",\n",
    "                            \"      <td>1</td>\\n\",\n",
    "                            \"      <td>3</td>\\n\",\n",
    "                            \"      <td>4</td>\\n\",\n",
    "                            \"      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\\n\",\n",
    "                            \"      <td>What would happen if the Indian government sto...</td>\\n\",\n",
    "                            \"      <td>0</td>\\n\",\n",
    "                            \"    </tr>\\n\",\n",
    "                            \"    <tr>\\n\",\n",
    "                            \"      <th>2</th>\\n\",\n",
    "                            \"      <td>2</td>\\n\",\n",
    "                            \"      <td>5</td>\\n\",\n",
    "                            \"      <td>6</td>\\n\",\n",
    "                            \"      <td>How can I increase the speed of my internet co...</td>\\n\",\n",
    "                            \"      <td>How can Internet speed be increased by hacking...</td>\\n\",\n",
    "                            \"      <td>0</td>\\n\",\n",
    "                            \"    </tr>\\n\",\n",
    "                            \"    <tr>\\n\",\n",
    "                            \"      <th>3</th>\\n\",\n",
    "                            \"      <td>3</td>\\n\",\n",
    "                            \"      <td>7</td>\\n\",\n",
    "                            \"      <td>8</td>\\n\",\n",
    "                            \"      <td>Why am I mentally very lonely? How can I solve...</td>\\n\",\n",
    "                            \"      <td>Find the remainder when [math]23^{24}[/math] i...</td>\\n\",\n",
    "                            \"      <td>0</td>\\n\",\n",
    "                            \"    </tr>\\n\",\n",
    "                            \"    <tr>\\n\",\n",
    "                            \"      <th>4</th>\\n\",\n",
    "                            \"      <td>4</td>\\n\",\n",
    "                            \"      <td>9</td>\\n\",\n",
    "                            \"      <td>10</td>\\n\",\n",
    "                            \"      <td>Which one dissolve in water quikly sugar, salt...</td>\\n\",\n",
    "                            \"      <td>Which fish would survive in salt water?</td>\\n\",\n",
    "                            \"      <td>0</td>\\n\",\n",
    "                            \"    </tr>\\n\",\n",
    "                            \"  </tbody>\\n\",\n",
    "                            \"</table>\\n\",\n",
    "                            \"</div>\"\n",
    "                        ],\n",
    "                        \"text/plain\": [\n",
    "                            \"   id  qid1  qid2                                          question1  \\\\\\n\",\n",
    "                            \"0   0     1     2  What is the step by step guide to invest in sh...   \\n\",\n",
    "                            \"1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \\n\",\n",
    "                            \"2   2     5     6  How can I increase the speed of my internet co...   \\n\",\n",
    "                            \"3   3     7     8  Why am I mentally very lonely? How can I solve...   \\n\",\n",
    "                            \"4   4     9    10  Which one dissolve in water quikly sugar, salt...   \\n\",\n",
    "                            \"\\n\",\n",
    "                            \"                                           question2  is_duplicate  \\n\",\n",
    "                            \"0  What is the step by step guide to invest in sh...             0  \\n\",\n",
    "                            \"1  What would happen if the Indian government sto...             0  \\n\",\n",
    "                            \"2  How can Internet speed be increased by hacking...             0  \\n\",\n",
    "                            \"3  Find the remainder when [math]23^{24}[/math] i...             0  \\n\",\n",
    "                            \"4            Which fish would survive in salt water?             0  \"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"execution_count\": 2,\n",
    "                    \"metadata\": {},\n",
    "                    \"output_type\": \"execute_result\"\n",
    "                }\n",
    "            ],\n",
    "            \"source\": [\n",
    "                \"df = pd.read_csv(\\\"questions.csv\\\")\\n\",\n",
    "                \"df.head()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 3,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"data\": {\n",
    "                        \"text/plain\": [\n",
    "                            \"(404351, 6)\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"execution_count\": 3,\n",
    "                    \"metadata\": {},\n",
    "                    \"output_type\": \"execute_result\"\n",
    "                }\n",
    "            ],\n",
    "            \"source\": [\n",
    "                \"df.shape\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 4,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"data\": {\n",
    "                        \"text/plain\": [\n",
    "                            \"(100000, 6)\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"execution_count\": 4,\n",
    "                    \"metadata\": {},\n",
    "                    \"output_type\": \"execute_result\"\n",
    "                }\n",
    "            ],\n",
    "            \"source\": [\n",
    "                \"#pick a sample of 5000 distinct random rows\\n\",\n",
    "                \"df = df.sample(n=100000, random_state=1)\\n\",\n",
    "                \"df.shape\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 5,\n",
    "            \"metadata\": {\n",
    "                \"execution\": {\n",
    "                    \"iopub.execute_input\": \"2021-12-07T01:21:29.482516Z\",\n",
    "                    \"iopub.status.busy\": \"2021-12-07T01:21:29.482071Z\",\n",
    "                    \"iopub.status.idle\": \"2021-12-07T01:21:29.56485Z\",\n",
    "                    \"shell.execute_reply\": \"2021-12-07T01:21:29.564017Z\",\n",
    "                    \"shell.execute_reply.started\": \"2021-12-07T01:21:29.482481Z\"\n",
    "                },\n",
    "                \"trusted\": true\n",
    "            },\n",
    "            \"outputs\": [],\n",
    "            \"source\": [\n",
    "                \"# question_1, question_2 = df['question1'].to_list(), df['question2'].to_list()\\n\",\n",
    "                \"# is_duplicate = df['is_duplicate'].to_list()\\n\",\n",
    "                \"# preprocess_neural(question_1, question_2, is_duplicate)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 6,\n",
    "            \"metadata\": {\n",
    "                \"trusted\": true\n",
    "            },\n",
    "            \"outputs\": [],\n",
    "            \"source\": [\n",
    "                \"q1_inp, q2_inp, is_duplicate = df['question1'].to_list(), df['question2'].to_list(), df['is_duplicate'].to_list()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"markdown\",\n",
    "            \"metadata\": {},\n",
    "            \"source\": [\n",
    "                \"Acquired Test data\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 7,\n",
    "            \"metadata\": {\n",
    "                \"trusted\": true\n",
    "            },\n",
    "            \"outputs\": [],\n",
    "            \"source\": [\n",
    "                \"MAX_WORDS_VOCAB = 200000\\n\",\n",
    "                \"tokenizer = Tokenizer(num_words = MAX_WORDS_VOCAB, lower=False, split=\\\" \\\")\\n\",\n",
    "                \"tokenizer.fit_on_texts(list(df['question1'].values.astype(str))+list(df['question2'].values.astype(str)))\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 8,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"stdout\",\n",
    "                    \"output_type\": \"stream\",\n",
    "                    \"text\": [\n",
    "                        \"Number of words in vocabulary:  60949\\n\"\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"source\": [\n",
    "                \"print(\\\"Number of words in vocabulary: \\\", len(tokenizer.word_index))\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 9,\n",
    "            \"metadata\": {\n",
    "                \"trusted\": true\n",
    "            },\n",
    "            \"outputs\": [],\n",
    "            \"source\": [\n",
    "                \"\\n\",\n",
    "                \"q1_sequence = tokenizer.texts_to_sequences(df['question1'].values.astype(str))\\n\",\n",
    "                \"q1_sequence = pad_sequences(q1_sequence, maxlen = 128)\\n\",\n",
    "                \"\\n\",\n",
    "                \"q2_sequence = tokenizer.texts_to_sequences(df['question2'].values.astype(str))\\n\",\n",
    "                \"q2_sequence = pad_sequences(q2_sequence, maxlen = 128)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 10,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [],\n",
    "            \"source\": [\n",
    "                \"windex = tokenizer.word_index\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 11,\n",
    "            \"metadata\": {\n",
    "                \"trusted\": true\n",
    "            },\n",
    "            \"outputs\": [],\n",
    "            \"source\": [\n",
    "                \"embedding_index = {}\\n\",\n",
    "                \"with open('glove.6B.300d.txt','r') as f:\\n\",\n",
    "                \"    for line in f:\\n\",\n",
    "                \"        values = line.split()\\n\",\n",
    "                \"        word = values[0]\\n\",\n",
    "                \"        vectors = np.asarray(values[1:], 'float32')\\n\",\n",
    "                \"        embedding_index[word] = vectors\\n\",\n",
    "                \"    f.close()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 12,\n",
    "            \"metadata\": {\n",
    "                \"trusted\": true\n",
    "            },\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"stdout\",\n",
    "                    \"output_type\": \"stream\",\n",
    "                    \"text\": [\n",
    "                        \"(60950, 300)\\n\"\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"source\": [\n",
    "                \"embedding_matrix = np.random.random((len(windex)+1, 300))\\n\",\n",
    "                \"\\n\",\n",
    "                \"for word, i in windex.items():\\n\",\n",
    "                \"    embedding_vector = embedding_index.get(word)\\n\",\n",
    "                \"    if embedding_vector is not None:\\n\",\n",
    "                \"        embedding_matrix[i] = embedding_vector\\n\",\n",
    "                \"\\n\",\n",
    "                \"print(embedding_matrix.shape)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 13,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"stdout\",\n",
    "                    \"output_type\": \"stream\",\n",
    "                    \"text\": [\n",
    "                        \"60950\\n\"\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"source\": [\n",
    "                \"print(len(windex)+1)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 14,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [],\n",
    "            \"source\": [\n",
    "                \"#split the data into 70-20-10 train-validation-test with random state 42\\n\",\n",
    "                \"from sklearn.model_selection import train_test_split\\n\",\n",
    "                \"q1_train, q1_test, q2_train, q2_test, y_train, y_test = train_test_split(q1_sequence, q2_sequence, is_duplicate, test_size=0.1, random_state=42)\\n\",\n",
    "                \"q1_train, q1_val, q2_train, q2_val, y_train, y_val = train_test_split(q1_train, q2_train, y_train, test_size=0.2, random_state=42)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 15,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [],\n",
    "            \"source\": [\n",
    "                \"import tensorflow as tf\\n\",\n",
    "                \"y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\\n\",\n",
    "                \"y_val = tf.keras.utils.to_categorical(y_val, num_classes=2)\\n\",\n",
    "                \"y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 16,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"stdout\",\n",
    "                    \"output_type\": \"stream\",\n",
    "                    \"text\": [\n",
    "                        \"(100000,)\\n\"\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"source\": [\n",
    "                \"is_duplicate = np.array(is_duplicate)\\n\",\n",
    "                \"print(is_duplicate.shape)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 17,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"stdout\",\n",
    "                    \"output_type\": \"stream\",\n",
    "                    \"text\": [\n",
    "                        \"Train:  [0.63294444 0.36705556]\\n\",\n",
    "                        \"Validation:  [0.6358333  0.36416668]\\n\",\n",
    "                        \"Test:  [0.6291 0.3709]\\n\"\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"source\": [\n",
    "                \"#print the ratio of positive and negative samples in train, validation and test\\n\",\n",
    "                \"y_train, y_val, y_test = np.array(y_train), np.array(y_val), np.array(y_test)\\n\",\n",
    "                \"print(\\\"Train: \\\", sum(y_train)/len(y_train))\\n\",\n",
    "                \"print(\\\"Validation: \\\", sum(y_val)/len(y_val))\\n\",\n",
    "                \"print(\\\"Test: \\\", sum(y_test)/len(y_test))\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 107,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [],\n",
    "            \"source\": [\n",
    "                \"# %load models.py\\n\",\n",
    "                \"from keras.layers import *\\n\",\n",
    "                \"from keras.models import Model\\n\",\n",
    "                \"import tensorflow as tf\\n\",\n",
    "                \"import numpy as np \\n\",\n",
    "                \"from keras import Sequential\\n\",\n",
    "                \"from keras.optimizers import adam_v2\\n\",\n",
    "                \"import keras as K\\n\",\n",
    "                \"\\n\",\n",
    "                \"class NeuralModels:\\n\",\n",
    "                \"    def __init__(self, emb_mat, vocab_size = -1, loss = \\\"binary_crossentropy\\\", epochs = 10, optimizer = \\\"adam\\\", metrics = [\\\"accuracy\\\"]):\\n\",\n",
    "                \"        self.epochs = epochs\\n\",\n",
    "                \"        self.optimizer = optimizer\\n\",\n",
    "                \"        self.loss = loss\\n\",\n",
    "                \"        self.metrics = metrics\\n\",\n",
    "                \"        self.vocab_size = vocab_size\\n\",\n",
    "                \"        self.embedding_matrix = emb_mat\\n\",\n",
    "                \"        self.model = Sequential()\\n\",\n",
    "                \"\\n\",\n",
    "                \"class CBOW(NeuralModels):\\n\",\n",
    "                \"    def __init__(self, emb_mat, vocab_size = -1, loss = \\\"binary_crossentropy\\\", epochs = 10, optimizer = \\\"adam\\\", metrics = [\\\"accuracy\\\"]):\\n\",\n",
    "                \"        super().__init__(emb_mat, vocab_size, loss, epochs, optimizer, metrics)\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    def fit(self,xtrain, xval, ytrain, yval):\\n\",\n",
    "                \"        self.model.add(Dense(300, input_shape = (900,), activation = \\\"sigmoid\\\"))\\n\",\n",
    "                \"        self.model.add(Dropout(0.1))\\n\",\n",
    "                \"        self.model.add(Dense(200, activation = \\\"sigmoid\\\"))\\n\",\n",
    "                \"        self.model.add(Dropout(0.1))\\n\",\n",
    "                \"        self.model.add(Dense(100, activation = \\\"sigmoid\\\"))\\n\",\n",
    "                \"        self.model.add(Dropout(0.1))\\n\",\n",
    "                \"        self.model.add(Dense(2, activation = \\\"softmax\\\"))\\n\",\n",
    "                \"        self.model.compile(loss = self.loss, optimizer = self.optimizer, metrics = self.metrics)\\n\",\n",
    "                \"        self.model.fit(xtrain, ytrain, epochs = self.epochs, validation_data = (xval, yval), batch_size = 64, verbose = 1)\\n\",\n",
    "                \"\\n\",\n",
    "                \"    def get_model_summary(self):\\n\",\n",
    "                \"        self.model.summary()\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    def predict(self, xtest):\\n\",\n",
    "                \"        return self.model.predict(xtest)\\n\",\n",
    "                \"\\n\",\n",
    "                \"class LsTM(NeuralModels):\\n\",\n",
    "                \"    def __init__(self, emb_mat, vocab_size = -1, loss = \\\"binary_crossentropy\\\", epochs = 10, optimizer = \\\"adam\\\", metrics = [\\\"accuracy\\\"]):\\n\",\n",
    "                \"        super().__init__(emb_mat, vocab_size, loss, epochs, optimizer, metrics)\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    def train_model(self):\\n\",\n",
    "                \"        inp1 = Input(shape = (128,))\\n\",\n",
    "                \"        inp2 = Input(shape = (128,))\\n\",\n",
    "                \"        emb1 = Embedding(output_dim=300, weights = [self.embedding_matrix], trainable = False, input_dim=self.vocab_size, input_length=128)(inp1)\\n\",\n",
    "                \"        emb2 = Embedding(output_dim=300, weights = [self.embedding_matrix], trainable = False, input_dim=self.vocab_size, input_length=128)(inp2)\\n\",\n",
    "                \"        concat = Concatenate(axis = -1)([emb1 + emb2, emb1 - emb2, emb1 * emb2])\\n\",\n",
    "                \"        lstm = LSTM(150, return_sequences=False, dropout=0.2, return_state=True)(concat)\\n\",\n",
    "                \"        out = Dense(2, activation = \\\"softmax\\\")(lstm[2])\\n\",\n",
    "                \"        self.model = Model(inputs = [inp1, inp2], outputs = out)\\n\",\n",
    "                \"        self.model.compile(loss = self.loss, optimizer = adam_v2.Adam(learning_rate=0.0008), metrics = self.metrics)\\n\",\n",
    "                \"\\n\",\n",
    "                \"    def get_model_summary(self):\\n\",\n",
    "                \"        self.model.summary()\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    def predict(self, xtest):\\n\",\n",
    "                \"        return self.model.predict(xtest)\\n\",\n",
    "                \"\\n\",\n",
    "                \"class BiLSTM(NeuralModels):\\n\",\n",
    "                \"    def __init__(self, emb_mat, vocab_size = -1, loss = \\\"binary_crossentropy\\\", epochs = 10, optimizer = \\\"adam\\\", metrics = [\\\"accuracy\\\"]):\\n\",\n",
    "                \"        super().__init__(emb_mat, vocab_size, loss, epochs, optimizer, metrics)\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    def train_model(self):\\n\",\n",
    "                \"        inp1 = Input(shape = (128,))\\n\",\n",
    "                \"        inp2 = Input(shape = (128,))\\n\",\n",
    "                \"        emb1 = Embedding(output_dim=300, weights = [self.embedding_matrix], trainable = False, input_dim=self.vocab_size, input_length=128)(inp1)\\n\",\n",
    "                \"        emb2 = Embedding(output_dim=300, weights = [self.embedding_matrix], trainable = False, input_dim=self.vocab_size, input_length=128)(inp2)\\n\",\n",
    "                \"        concat = Concatenate(axis = -1)([emb1 + emb2, emb1 - emb2, emb1 * emb2])\\n\",\n",
    "                \"        out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, kernel_regularizer='l2', dropout=0.1, return_sequences=True))(concat)\\n\",\n",
    "                \"        out = tf.keras.backend.mean(out, axis=1, keepdims=False)\\n\",\n",
    "                \"        output = tf.keras.layers.Dense(2, kernel_regularizer='l2', activation='softmax')(out)\\n\",\n",
    "                \"        self.model = Model(inputs = [inp1, inp2], outputs = output)\\n\",\n",
    "                \"        self.model.compile(loss = self.loss, optimizer = self.optimizer, metrics = self.metrics)\\n\",\n",
    "                \"\\n\",\n",
    "                \"    def get_model_summary(self):\\n\",\n",
    "                \"        self.model.summary()\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    def predict(self, xtest):\\n\",\n",
    "                \"        return self.model.predict(xtest)\\n\",\n",
    "                \"\\n\",\n",
    "                \"class LsTM_Attention(NeuralModels):\\n\",\n",
    "                \"    def __init__(self, emb_mat, vocab_size = -1, loss = \\\"binary_crossentropy\\\", epochs = 10, optimizer = \\\"adam\\\", metrics = [\\\"accuracy\\\"]):\\n\",\n",
    "                \"        super().__init__(emb_mat, vocab_size, loss, epochs, optimizer, metrics)\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    def train_model(self):\\n\",\n",
    "                \"        inp1 = Input(shape = (128,))\\n\",\n",
    "                \"        inp2 = Input(shape = (128,))\\n\",\n",
    "                \"        emb1 = Embedding(output_dim=300, weights = [self.embedding_matrix], trainable = False, input_dim=self.vocab_size, input_length=128)(inp1)\\n\",\n",
    "                \"        emb2 = Embedding(output_dim=300, weights = [self.embedding_matrix], trainable = False, input_dim=self.vocab_size, input_length=128)(inp2)\\n\",\n",
    "                \"        \\n\",\n",
    "                \"        lstm1 = LSTM(150, return_sequences=True, dropout=0.1, return_state=True)(emb1)\\n\",\n",
    "                \"        lstm2 = LSTM(150, return_sequences=True, dropout=0.1, return_state=True)(emb2)\\n\",\n",
    "                \"\\n\",\n",
    "                \"        attention = dot([lstm1[0], lstm2[0]], axes=[2, 2])\\n\",\n",
    "                \"        u_norm = Softmax(axis=-1)(attention)\\n\",\n",
    "                \"        v_norm = Softmax(axis=1)(attention)\\n\",\n",
    "                \"\\n\",\n",
    "                \"\\n\",\n",
    "                \"        u = dot([u_norm, lstm1[0]], axes=[1, 1])\\n\",\n",
    "                \"        v = dot([v_norm, lstm2[0]], axes=[1, 1])\\n\",\n",
    "                \"\\n\",\n",
    "                \"        WU_bar = Dense(150)(u[:, -1, :])\\n\",\n",
    "                \"        WV_bar = Dense(150)(v[:, -1, :])\\n\",\n",
    "                \"        VU = Dense(150)(lstm1[0][:, -1, :])\\n\",\n",
    "                \"        VV = Dense(150)(lstm2[0][:, -1, :])\\n\",\n",
    "                \"\\n\",\n",
    "                \"        ufinal = Add()([WU_bar, VU])\\n\",\n",
    "                \"        vfinal = Add()([WV_bar, VV])\\n\",\n",
    "                \"\\n\",\n",
    "                \"        ufinal = Activation('tanh')(ufinal)\\n\",\n",
    "                \"        vfinal = Activation('tanh')(vfinal)\\n\",\n",
    "                \"        \\n\",\n",
    "                \"        concat = Concatenate(axis = -1)([ufinal, vfinal])\\n\",\n",
    "                \"        out = Dense(2, activation = \\\"softmax\\\")(concat)\\n\",\n",
    "                \"        self.model = Model(inputs = [inp1, inp2], outputs = out)\\n\",\n",
    "                \"        self.model.compile(loss = self.loss, optimizer = self.optimizer, metrics = self.metrics)\\n\",\n",
    "                \"\\n\",\n",
    "                \"\\n\",\n",
    "                \"    def get_model_summary(self):\\n\",\n",
    "                \"        self.model.summary()\\n\",\n",
    "                \"    \\n\",\n",
    "                \"    def predict(self, xtest):\\n\",\n",
    "                \"        return self.model.predict(xtest)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 108,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [],\n",
    "            \"source\": [\n",
    "                \"model = LsTM_Attention(embedding_matrix, len(windex) + 1, loss=\\\"categorical_crossentropy\\\")\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 109,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [],\n",
    "            \"source\": [\n",
    "                \"model.train_model()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 110,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"stdout\",\n",
    "                    \"output_type\": \"stream\",\n",
    "                    \"text\": [\n",
    "                        \"Model: \\\"model_6\\\"\\n\",\n",
    "                        \"__________________________________________________________________________________________________\\n\",\n",
    "                        \" Layer (type)                   Output Shape         Param #     Connected to                     \\n\",\n",
    "                        \"==================================================================================================\\n\",\n",
    "                        \" input_45 (InputLayer)          [(None, 128)]        0           []                               \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" input_46 (InputLayer)          [(None, 128)]        0           []                               \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" embedding_44 (Embedding)       (None, 128, 300)     18285000    ['input_45[0][0]']               \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" embedding_45 (Embedding)       (None, 128, 300)     18285000    ['input_46[0][0]']               \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" lstm_40 (LSTM)                 [(None, 128, 150),   270600      ['embedding_44[0][0]']           \\n\",\n",
    "                        \"                                 (None, 150),                                                     \\n\",\n",
    "                        \"                                 (None, 150)]                                                     \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" lstm_41 (LSTM)                 [(None, 128, 150),   270600      ['embedding_45[0][0]']           \\n\",\n",
    "                        \"                                 (None, 150),                                                     \\n\",\n",
    "                        \"                                 (None, 150)]                                                     \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" dot_24 (Dot)                   (None, 128, 128)     0           ['lstm_40[0][0]',                \\n\",\n",
    "                        \"                                                                  'lstm_41[0][0]']                \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" softmax_4 (Softmax)            (None, 128, 128)     0           ['dot_24[0][0]']                 \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" softmax_5 (Softmax)            (None, 128, 128)     0           ['dot_24[0][0]']                 \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" dot_25 (Dot)                   (None, 128, 150)     0           ['softmax_4[0][0]',              \\n\",\n",
    "                        \"                                                                  'lstm_40[0][0]']                \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" dot_26 (Dot)                   (None, 128, 150)     0           ['softmax_5[0][0]',              \\n\",\n",
    "                        \"                                                                  'lstm_41[0][0]']                \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" tf.__operators__.getitem_281 (  (None, 150)         0           ['dot_25[0][0]']                 \\n\",\n",
    "                        \" SlicingOpLambda)                                                                                 \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" tf.__operators__.getitem_283 (  (None, 150)         0           ['lstm_40[0][0]']                \\n\",\n",
    "                        \" SlicingOpLambda)                                                                                 \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" tf.__operators__.getitem_282 (  (None, 150)         0           ['dot_26[0][0]']                 \\n\",\n",
    "                        \" SlicingOpLambda)                                                                                 \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" tf.__operators__.getitem_284 (  (None, 150)         0           ['lstm_41[0][0]']                \\n\",\n",
    "                        \" SlicingOpLambda)                                                                                 \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" dense_11 (Dense)               (None, 150)          22650       ['tf.__operators__.getitem_281[0]\\n\",\n",
    "                        \"                                                                 [0]']                            \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" dense_13 (Dense)               (None, 150)          22650       ['tf.__operators__.getitem_283[0]\\n\",\n",
    "                        \"                                                                 [0]']                            \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" dense_12 (Dense)               (None, 150)          22650       ['tf.__operators__.getitem_282[0]\\n\",\n",
    "                        \"                                                                 [0]']                            \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" dense_14 (Dense)               (None, 150)          22650       ['tf.__operators__.getitem_284[0]\\n\",\n",
    "                        \"                                                                 [0]']                            \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" add_2 (Add)                    (None, 150)          0           ['dense_11[0][0]',               \\n\",\n",
    "                        \"                                                                  'dense_13[0][0]']               \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" add_3 (Add)                    (None, 150)          0           ['dense_12[0][0]',               \\n\",\n",
    "                        \"                                                                  'dense_14[0][0]']               \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" activation_517 (Activation)    (None, 150)          0           ['add_2[0][0]']                  \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" activation_518 (Activation)    (None, 150)          0           ['add_3[0][0]']                  \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" concatenate_9 (Concatenate)    (None, 300)          0           ['activation_517[0][0]',         \\n\",\n",
    "                        \"                                                                  'activation_518[0][0]']         \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \" dense_15 (Dense)               (None, 2)            602         ['concatenate_9[0][0]']          \\n\",\n",
    "                        \"                                                                                                  \\n\",\n",
    "                        \"==================================================================================================\\n\",\n",
    "                        \"Total params: 37,202,402\\n\",\n",
    "                        \"Trainable params: 632,402\\n\",\n",
    "                        \"Non-trainable params: 36,570,000\\n\",\n",
    "                        \"__________________________________________________________________________________________________\\n\"\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"source\": [\n",
    "                \"model.get_model_summary()\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 111,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"stdout\",\n",
    "                    \"output_type\": \"stream\",\n",
    "                    \"text\": [\n",
    "                        \"(18000, 128)\\n\"\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"source\": [\n",
    "                \"print(q1_val.shape)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 112,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"stdout\",\n",
    "                    \"output_type\": \"stream\",\n",
    "                    \"text\": [\n",
    "                        \"Epoch 1/4\\n\",\n",
    "                        \"9000/9000 [==============================] - 1253s 139ms/step - loss: 0.5809 - accuracy: 0.6902 - val_loss: 0.5423 - val_accuracy: 0.7200\\n\",\n",
    "                        \"Epoch 2/4\\n\",\n",
    "                        \"9000/9000 [==============================] - 1242s 138ms/step - loss: 0.5190 - accuracy: 0.7422 - val_loss: 0.5348 - val_accuracy: 0.7373\\n\",\n",
    "                        \"Epoch 3/4\\n\",\n",
    "                        \"9000/9000 [==============================] - 1238s 138ms/step - loss: 0.4668 - accuracy: 0.7751 - val_loss: 0.5124 - val_accuracy: 0.7573\\n\",\n",
    "                        \"Epoch 4/4\\n\",\n",
    "                        \"9000/9000 [==============================] - 1248s 139ms/step - loss: 0.4030 - accuracy: 0.8133 - val_loss: 0.4988 - val_accuracy: 0.7642\\n\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"data\": {\n",
    "                        \"text/plain\": [\n",
    "                            \"<keras.callbacks.History at 0x7f30642141c0>\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"execution_count\": 112,\n",
    "                    \"metadata\": {},\n",
    "                    \"output_type\": \"execute_result\"\n",
    "                }\n",
    "            ],\n",
    "            \"source\": [\n",
    "                \"model.model.fit([q1_train, q2_train], y_train, epochs = 4, validation_data = ([q1_val, q2_val], y_val), batch_size = 8, validation_batch_size=4, verbose = 1)\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": 113,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [\n",
    "                {\n",
    "                    \"name\": \"stdout\",\n",
    "                    \"output_type\": \"stream\",\n",
    "                    \"text\": [\n",
    "                        \"313/313 [==============================] - 31s 87ms/step\\n\",\n",
    "                        \"Accuracy:  0.7751\\n\",\n",
    "                        \"F1 Score:  0.7064308681672025\\n\"\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"source\": [\n",
    "                \"y_pred = model.predict([q1_test, q2_test])\\n\",\n",
    "                \"y_pred1d, y_actual1d = [], []\\n\",\n",
    "                \"for i in range(len(y_test)):\\n\",\n",
    "                \"    if(y_test[i][0] == 1):\\n\",\n",
    "                \"        y_actual1d.append(0)\\n\",\n",
    "                \"    else:\\n\",\n",
    "                \"        y_actual1d.append(1)\\n\",\n",
    "                \"\\n\",\n",
    "                \"for i in range(len(y_pred)):\\n\",\n",
    "                \"    if(y_pred[i][0] > y_pred[i][1]):\\n\",\n",
    "                \"        y_pred1d.append(0)\\n\",\n",
    "                \"    else:\\n\",\n",
    "                \"        y_pred1d.append(1)\\n\",\n",
    "                \"\\n\",\n",
    "                \"from sklearn.metrics import accuracy_score, f1_score\\n\",\n",
    "                \"print(\\\"Accuracy: \\\", accuracy_score(y_actual1d, y_pred1d))\\n\",\n",
    "                \"print(\\\"F1 Score: \\\", f1_score(y_actual1d, y_pred1d))\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"cell_type\": \"code\",\n",
    "            \"execution_count\": null,\n",
    "            \"metadata\": {},\n",
    "            \"outputs\": [],\n",
    "            \"source\": []\n",
    "        }\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"kernelspec\": {\n",
    "            \"display_name\": \"Python 3.10.4 64-bit\",\n",
    "            \"language\": \"python\",\n",
    "            \"name\": \"python3\"\n",
    "        },\n",
    "        \"language_info\": {\n",
    "            \"codemirror_mode\": {\n",
    "                \"name\": \"ipython\",\n",
    "                \"version\": 3\n",
    "            },\n",
    "            \"file_extension\": \".py\",\n",
    "            \"mimetype\": \"text/x-python\",\n",
    "            \"name\": \"python\",\n",
    "            \"nbconvert_exporter\": \"python\",\n",
    "            \"pygments_lexer\": \"ipython3\",\n",
    "            \"version\": \"3.10.4\"\n",
    "        },\n",
    "        \"vscode\": {\n",
    "            \"interpreter\": {\n",
    "                \"hash\": \"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"nbformat\": 4,\n",
    "    \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
