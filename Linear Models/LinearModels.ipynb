{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"[nltk_data] Downloading package punkt to /home/jeopardy/nltk_data...\\n\",\n",
    "      \"[nltk_data]   Package punkt is already up-to-date!\\n\",\n",
    "      \"[nltk_data] Downloading package stopwords to\\n\",\n",
    "      \"[nltk_data]     /home/jeopardy/nltk_data...\\n\",\n",
    "      \"[nltk_data]   Package stopwords is already up-to-date!\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import numpy as np \\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"import string\\n\",\n",
    "    \"from nltk.stem.porter import PorterStemmer\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"nltk.download('punkt')\\n\",\n",
    "    \"nltk.download('stopwords')\\n\",\n",
    "    \"stemmer = PorterStemmer()\\n\",\n",
    "    \"stopwords = set(nltk.corpus.stopwords.words('english'))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 29,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Ratio of duplicate questions in the splits\\n\",\n",
    "      \"Train set:  0.36919749967314835\\n\",\n",
    "      \"Validation set:  0.36920279997031835\\n\",\n",
    "      \"Test set:  0.369190432610255\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# load data\\n\",\n",
    "    \"df = pd.read_csv('questions.csv.zip')\\n\",\n",
    "    \"df.dropna(how=\\\"any\\\").reset_index(drop=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# train validate test split 70:20:10\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"X_train_q1, X_test_q1, X_train_q2, X_test_q2, y_train, y_test = train_test_split(df['question1'], df['question2'], df['is_duplicate'], test_size=0.3, random_state=42, stratify=df['is_duplicate'])\\n\",\n",
    "    \"X_val_q1, X_test_q1, X_val_q2, X_test_q2, y_val, y_test = train_test_split(X_test_q1, X_test_q2, y_test, test_size=(1/3), random_state=42, stratify=y_test)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Ratio of duplicate questions in the splits\\n\",\n",
    "      \"Train set:  0.36919749967314835\\n\",\n",
    "      \"Validation set:  0.36920279997031835\\n\",\n",
    "      \"Test set:  0.369190432610255\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"y_train, y_val, y_test = np.array(y_train), np.array(y_val), np.array(y_test)\\n\",\n",
    "    \"# ratio of duplicate questions in train, validation and test set\\n\",\n",
    "    \"print(\\\"Ratio of duplicate questions in the splits\\\")\\n\",\n",
    "    \"print(\\\"Train set: \\\", y_train.sum()/len(y_train))\\n\",\n",
    "    \"print(\\\"Validation set: \\\", y_val.sum()/len(y_val))\\n\",\n",
    "    \"print(\\\"Test set: \\\", y_test.sum()/len(y_test))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"INPUT = './input/'\\n\",\n",
    "    \"TRAIN_LINEAR_PATH = INPUT + 'train_linear.csv.zip'\\n\",\n",
    "    \"TEST_LINEAR_PATH = INPUT + 'test_linear.csv.zip'\\n\",\n",
    "    \"VAL_LINEAR_PATH = INPUT + 'val_linear.csv.zip'\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"train_linear = pd.DataFrame({'question1': X_train_q1, 'question2': X_train_q2, 'is_duplicate': y_train})\\n\",\n",
    "    \"val_linear = pd.DataFrame({'question1': X_val_q1, 'question2': X_val_q2, 'is_duplicate': y_val})\\n\",\n",
    "    \"test_linear = pd.DataFrame({'question1': X_test_q1, 'question2': X_test_q2, 'is_duplicate': y_test})\\n\",\n",
    "    \"allQuestions = pd.concat((train_linear['question1'], train_linear['question2'])).reset_index(drop=True).astype(str)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 22,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# dump split files\\n\",\n",
    "    \"os.makedirs(INPUT, exist_ok=True)  \\n\",\n",
    "    \"train_linear.to_csv(TRAIN_LINEAR_PATH, index=False, compression='zip')\\n\",\n",
    "    \"val_linear.to_csv(VAL_LINEAR_PATH, index=False, compression='zip')\\n\",\n",
    "    \"test_linear.to_csv(TEST_LINEAR_PATH, index=False, compression='zip')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"train_linear = pd.read_csv(TRAIN_LINEAR_PATH)\\n\",\n",
    "    \"val_linear = pd.read_csv(VAL_LINEAR_PATH)\\n\",\n",
    "    \"test_linear = pd.read_csv(TEST_LINEAR_PATH)\\n\",\n",
    "    \"X_train_q1, X_train_q2, y_train = train_linear['question1'].astype('U').values, train_linear['question2'].astype('U').values, train_linear['is_duplicate'].values\\n\",\n",
    "    \"X_val_q1, X_val_q2, y_val = val_linear['question1'].astype('U').values, val_linear['question2'].astype('U').values, val_linear['is_duplicate'].values\\n\",\n",
    "    \"X_test_q1, X_test_q2, y_test = test_linear['question1'].astype('U').values, test_linear['question2'].astype('U').values, test_linear['is_duplicate'].values\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.feature_extraction.text import CountVectorizer\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\\n\",\n",
    "    \"from sklearn.linear_model import SGDClassifier\\n\",\n",
    "    \"from sklearn.svm import SVC, LinearSVC\\n\",\n",
    "    \"from sklearn.metrics import f1_score\\n\",\n",
    "    \"from sklearn.metrics import accuracy_score\\n\",\n",
    "    \"from scipy.sparse import hstack as sparse_hstack, vstack as sparse_vstack, save_npz, load_npz\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 32,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"stemmer = PorterStemmer()\\n\",\n",
    "    \"\\n\",\n",
    "    \"def tokenize(text: str) -> list[str]:\\n\",\n",
    "    \"    tokens = nltk.word_tokenize(re.sub(r'[^\\\\x00-\\\\x7F]+',' ', text))\\n\",\n",
    "    \"    tokens = [stemmer.stem(w) for w in tokens if stemmer.stem(w) not in stopwords]\\n\",\n",
    "    \"    return tokens\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"N_GRAMS_PATH = './n_gram_features/'\\n\",\n",
    "    \"UNIGRAM_PATH = N_GRAMS_PATH + 'unigrams_linear/'\\n\",\n",
    "    \"BIGRAM_PATH = N_GRAMS_PATH + 'bigrams_linear/'\\n\",\n",
    "    \"TRIGRAM_PATH = N_GRAMS_PATH + 'trigrams_linear/'\\n\",\n",
    "    \"os.makedirs(N_GRAMS_PATH, exist_ok=True)\\n\",\n",
    "    \"os.makedirs(UNIGRAM_PATH, exist_ok=True)\\n\",\n",
    "    \"os.makedirs(BIGRAM_PATH, exist_ok=True)\\n\",\n",
    "    \"os.makedirs(TRIGRAM_PATH, exist_ok=True)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Creating Unigram features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 44,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"unigramVectorizer = CountVectorizer(\\n\",\n",
    "    \"                        analyzer='word', \\n\",\n",
    "    \"                        ngram_range=(1,1), \\n\",\n",
    "    \"                        lowercase=True,\\n\",\n",
    "    \"                        tokenizer=tokenize\\n\",\n",
    "    \"                    )\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"unigramVectorizer.fit(allQuestions)\\n\",\n",
    "    \"q1_train = unigramVectorizer.transform(train_linear['question1'].astype(str))\\n\",\n",
    "    \"q2_train = unigramVectorizer.transform(train_linear['question2'].astype(str))\\n\",\n",
    "    \"X_train_unigram = sparse_hstack([q1_train, q2_train])\\n\",\n",
    "    \"q1_val = unigramVectorizer.transform(val_linear['question1'].astype(str))\\n\",\n",
    "    \"q2_val = unigramVectorizer.transform(val_linear['question2'].astype(str))\\n\",\n",
    "    \"X_val_unigram = sparse_hstack([q1_val, q2_val])\\n\",\n",
    "    \"q1_test = unigramVectorizer.transform(test_linear['question1'].astype(str))\\n\",\n",
    "    \"q2_test = unigramVectorizer.transform(test_linear['question2'].astype(str))\\n\",\n",
    "    \"X_test_unigram = sparse_hstack([q1_test, q2_test])\\n\",\n",
    "    \"\\n\",\n",
    "    \"save_npz(UNIGRAM_PATH + \\\"train.npz\\\", X_train_unigram)\\n\",\n",
    "    \"save_npz(UNIGRAM_PATH + \\\"val.npz\\\", X_val_unigram)\\n\",\n",
    "    \"save_npz(UNIGRAM_PATH + \\\"test.npz\\\", X_test_unigram)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Creating Bigram features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 45,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"bigramVectorizer = CountVectorizer(\\n\",\n",
    "    \"                        analyzer='word', \\n\",\n",
    "    \"                        ngram_range=(1,2), \\n\",\n",
    "    \"                        lowercase=True,\\n\",\n",
    "    \"                        tokenizer=tokenize\\n\",\n",
    "    \"                    )\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"bigramVectorizer.fit(allQuestions)\\n\",\n",
    "    \"q1_train = bigramVectorizer.transform(train_linear['question1'].astype(str))\\n\",\n",
    "    \"q2_train = bigramVectorizer.transform(train_linear['question2'].astype(str))\\n\",\n",
    "    \"X_train_bigram = sparse_hstack([q1_train, q2_train])\\n\",\n",
    "    \"q1_val = bigramVectorizer.transform(val_linear['question1'].astype(str))\\n\",\n",
    "    \"q2_val = bigramVectorizer.transform(val_linear['question2'].astype(str))\\n\",\n",
    "    \"X_val_bigram = sparse_hstack([q1_val, q2_val])\\n\",\n",
    "    \"q1_test = bigramVectorizer.transform(test_linear['question1'].astype(str))\\n\",\n",
    "    \"q2_test = bigramVectorizer.transform(test_linear['question2'].astype(str))\\n\",\n",
    "    \"X_test_bigram = sparse_hstack([q1_test, q2_test])\\n\",\n",
    "    \"\\n\",\n",
    "    \"save_npz(BIGRAM_PATH + \\\"train.npz\\\", X_train_bigram)\\n\",\n",
    "    \"save_npz(BIGRAM_PATH + \\\"val.npz\\\", X_val_bigram)\\n\",\n",
    "    \"save_npz(BIGRAM_PATH + \\\"test.npz\\\", X_test_bigram)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Creating Trigram features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 46,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"trigramVectorizer = CountVectorizer(\\n\",\n",
    "    \"                        analyzer='word', \\n\",\n",
    "    \"                        ngram_range=(1,3), \\n\",\n",
    "    \"                        lowercase=True,\\n\",\n",
    "    \"                        tokenizer=tokenize\\n\",\n",
    "    \"                    )\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"trigramVectorizer.fit(allQuestions)\\n\",\n",
    "    \"q1_train = trigramVectorizer.transform(train_linear['question1'].astype(str))\\n\",\n",
    "    \"q2_train = trigramVectorizer.transform(train_linear['question2'].astype(str))\\n\",\n",
    "    \"X_train_trigram = sparse_hstack([q1_train, q2_train])\\n\",\n",
    "    \"q1_val = trigramVectorizer.transform(val_linear['question1'].astype(str))\\n\",\n",
    "    \"q2_val = trigramVectorizer.transform(val_linear['question2'].astype(str))\\n\",\n",
    "    \"X_val_trigram = sparse_hstack([q1_val, q2_val])\\n\",\n",
    "    \"q1_test = trigramVectorizer.transform(test_linear['question1'].astype(str))\\n\",\n",
    "    \"q2_test = trigramVectorizer.transform(test_linear['question2'].astype(str))\\n\",\n",
    "    \"X_test_trigram = sparse_hstack([q1_test, q2_test])\\n\",\n",
    "    \"\\n\",\n",
    "    \"save_npz(TRIGRAM_PATH + \\\"train.npz\\\", X_train_trigram)\\n\",\n",
    "    \"save_npz(TRIGRAM_PATH + \\\"val.npz\\\", X_val_trigram)\\n\",\n",
    "    \"save_npz(TRIGRAM_PATH + \\\"test.npz\\\", X_test_trigram)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Logistic Regression\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Unigrams\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 53,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Unigram Logistic Regression Accuracy:  0.7418437260382399\\n\",\n",
    "      \"Unigram Logistic Regression F1 Score:  0.6310840903467534\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# X_train_unigram = load_npz(UNIGRAM_PATH + \\\"train.npz\\\")\\n\",\n",
    "    \"# X_test_unigram = load_npz(UNIGRAM_PATH + \\\"test.npz\\\")\\n\",\n",
    "    \"unigramLogisticRegressor = SGDClassifier(\\n\",\n",
    "    \"                            loss='log_loss', \\n\",\n",
    "    \"                            penalty='l2', \\n\",\n",
    "    \"                            alpha=0.00001, \\n\",\n",
    "    \"                            max_iter=1000,\\n\",\n",
    "    \"                            n_iter_no_change=20,\\n\",\n",
    "    \"                            learning_rate='optimal',\\n\",\n",
    "    \"                            n_jobs=-1,\\n\",\n",
    "    \"                            random_state=42)\\n\",\n",
    "    \"unigramLogisticRegressor.fit(X_train_unigram, y_train)\\n\",\n",
    "    \"y_pred_unigram_logistic = unigramLogisticRegressor.predict(X_test_unigram)\\n\",\n",
    "    \"print(\\\"Unigram Logistic Regression Accuracy: \\\", accuracy_score(y_test, y_pred_unigram_logistic))\\n\",\n",
    "    \"print(\\\"Unigram Logistic Regression F1 Score: \\\", f1_score(y_test, y_pred_unigram_logistic))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Bigrams\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 54,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"bigram Logistic Regression Accuracy:  0.7962106408765984\\n\",\n",
    "      \"bigram Logistic Regression F1 Score:  0.7066405554566494\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# X_train_bigram = load_npz(BIGRAM_PATH + \\\"train.npz\\\")\\n\",\n",
    "    \"# X_test_bigram = load_npz(BIGRAM_PATH + \\\"test.npz\\\")\\n\",\n",
    "    \"bigramLogisticRegressor = SGDClassifier(\\n\",\n",
    "    \"                            loss='log_loss', \\n\",\n",
    "    \"                            penalty='l2', \\n\",\n",
    "    \"                            alpha=0.00001, \\n\",\n",
    "    \"                            max_iter=1000,\\n\",\n",
    "    \"                            n_iter_no_change=20,\\n\",\n",
    "    \"                            learning_rate='optimal',\\n\",\n",
    "    \"                            n_jobs=-1,\\n\",\n",
    "    \"                            random_state=42)\\n\",\n",
    "    \"bigramLogisticRegressor.fit(X_train_bigram, y_train)\\n\",\n",
    "    \"y_pred_bigram_logistic = bigramLogisticRegressor.predict(X_test_bigram)\\n\",\n",
    "    \"print(\\\"bigram Logistic Regression Accuracy: \\\", accuracy_score(y_test, y_pred_bigram_logistic))\\n\",\n",
    "    \"print(\\\"bigram Logistic Regression F1 Score: \\\", f1_score(y_test, y_pred_bigram_logistic))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Trigrams\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"trigram Logistic Regression Accuracy:  0.8114472284746098\\n\",\n",
    "      \"trigram Logistic Regression F1 Score:  0.7147614593077642\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# X_train_trigram = load_npz(TRIGRAM_PATH + \\\"train.npz\\\")\\n\",\n",
    "    \"# X_test_trigram = load_npz(TRIGRAM_PATH + \\\"test.npz\\\")\\n\",\n",
    "    \"trigramLogisticRegressor = SGDClassifier(\\n\",\n",
    "    \"                            loss='log_loss', \\n\",\n",
    "    \"                            penalty='l2', \\n\",\n",
    "    \"                            alpha=0.00001, \\n\",\n",
    "    \"                            max_iter=1000,\\n\",\n",
    "    \"                            n_iter_no_change=20,\\n\",\n",
    "    \"                            learning_rate='optimal',\\n\",\n",
    "    \"                            n_jobs=-1,\\n\",\n",
    "    \"                            random_state=42)\\n\",\n",
    "    \"trigramLogisticRegressor.fit(X_train_trigram, y_train)\\n\",\n",
    "    \"y_pred_trigram_logistic = trigramLogisticRegressor.predict(X_test_trigram)\\n\",\n",
    "    \"print(\\\"trigram Logistic Regression Accuracy: \\\", accuracy_score(y_test, y_pred_trigram_logistic))\\n\",\n",
    "    \"print(\\\"trigram Logistic Regression F1 Score: \\\", f1_score(y_test, y_pred_trigram_logistic))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Trigrams Tuned\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Applying GridSearchCV on Trigrams model to get the best set of parameters\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 81,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \\\"▸\\\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \\\"▾\\\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \\\"\\\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \\\"\\\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \\\"\\\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\\\"sk-container-id-4\\\" class=\\\"sk-top-container\\\"><div class=\\\"sk-text-repr-fallback\\\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\\n\",\n",
    "       \"             estimator=SGDClassifier(loss=&#x27;log_loss&#x27;, n_jobs=-1,\\n\",\n",
    "       \"                                     random_state=42),\\n\",\n",
    "       \"             n_jobs=-1,\\n\",\n",
    "       \"             param_grid={&#x27;alpha&#x27;: [0.01, 0.001, 0.0001, 1e-05, 1e-06],\\n\",\n",
    "       \"                         &#x27;n_iter_no_change&#x27;: [5, 10, 15, 20]},\\n\",\n",
    "       \"             refit=&#x27;f1&#x27;, scoring=[&#x27;accuracy&#x27;, &#x27;f1&#x27;])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\\\"sk-container\\\" hidden><div class=\\\"sk-item sk-dashed-wrapped\\\"><div class=\\\"sk-label-container\\\"><div class=\\\"sk-label sk-toggleable\\\"><input class=\\\"sk-toggleable__control sk-hidden--visually\\\" id=\\\"sk-estimator-id-10\\\" type=\\\"checkbox\\\" ><label for=\\\"sk-estimator-id-10\\\" class=\\\"sk-toggleable__label sk-toggleable__label-arrow\\\">GridSearchCV</label><div class=\\\"sk-toggleable__content\\\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\\n\",\n",
    "       \"             estimator=SGDClassifier(loss=&#x27;log_loss&#x27;, n_jobs=-1,\\n\",\n",
    "       \"                                     random_state=42),\\n\",\n",
    "       \"             n_jobs=-1,\\n\",\n",
    "       \"             param_grid={&#x27;alpha&#x27;: [0.01, 0.001, 0.0001, 1e-05, 1e-06],\\n\",\n",
    "       \"                         &#x27;n_iter_no_change&#x27;: [5, 10, 15, 20]},\\n\",\n",
    "       \"             refit=&#x27;f1&#x27;, scoring=[&#x27;accuracy&#x27;, &#x27;f1&#x27;])</pre></div></div></div><div class=\\\"sk-parallel\\\"><div class=\\\"sk-parallel-item\\\"><div class=\\\"sk-item\\\"><div class=\\\"sk-label-container\\\"><div class=\\\"sk-label sk-toggleable\\\"><input class=\\\"sk-toggleable__control sk-hidden--visually\\\" id=\\\"sk-estimator-id-11\\\" type=\\\"checkbox\\\" ><label for=\\\"sk-estimator-id-11\\\" class=\\\"sk-toggleable__label sk-toggleable__label-arrow\\\">estimator: SGDClassifier</label><div class=\\\"sk-toggleable__content\\\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, n_jobs=-1, random_state=42)</pre></div></div></div><div class=\\\"sk-serial\\\"><div class=\\\"sk-item\\\"><div class=\\\"sk-estimator sk-toggleable\\\"><input class=\\\"sk-toggleable__control sk-hidden--visually\\\" id=\\\"sk-estimator-id-12\\\" type=\\\"checkbox\\\" ><label for=\\\"sk-estimator-id-12\\\" class=\\\"sk-toggleable__label sk-toggleable__label-arrow\\\">SGDClassifier</label><div class=\\\"sk-toggleable__content\\\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, n_jobs=-1, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\\n\",\n",
    "       \"             estimator=SGDClassifier(loss='log_loss', n_jobs=-1,\\n\",\n",
    "       \"                                     random_state=42),\\n\",\n",
    "       \"             n_jobs=-1,\\n\",\n",
    "       \"             param_grid={'alpha': [0.01, 0.001, 0.0001, 1e-05, 1e-06],\\n\",\n",
    "       \"                         'n_iter_no_change': [5, 10, 15, 20]},\\n\",\n",
    "       \"             refit='f1', scoring=['accuracy', 'f1'])\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 81,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"trigramLogisticRegressor = SGDClassifier(\\n\",\n",
    "    \"                            loss='log_loss',\\n\",\n",
    "    \"                            penalty='l2',\\n\",\n",
    "    \"                            max_iter=1000,\\n\",\n",
    "    \"                            learning_rate='optimal',\\n\",\n",
    "    \"                            n_jobs=-1,\\n\",\n",
    "    \"                            random_state=42)\\n\",\n",
    "    \"parameters = dict({\\n\",\n",
    "    \"                'alpha':[0.01, 0.001, 0.0001, 0.00001, 0.000001],\\n\",\n",
    "    \"                'n_iter_no_change': [5, 10, 15, 20]\\n\",\n",
    "    \"            })\\n\",\n",
    "    \"cv_stratified_splitter = StratifiedKFold(n_splits=5)\\n\",\n",
    "    \"grid_search = GridSearchCV(trigramLogisticRegressor, \\n\",\n",
    "    \"                            parameters, \\n\",\n",
    "    \"                            cv=cv_stratified_splitter, \\n\",\n",
    "    \"                            scoring=['accuracy', 'f1'], \\n\",\n",
    "    \"                            n_jobs=-1,\\n\",\n",
    "    \"                            refit='f1')\\n\",\n",
    "    \"grid_search.fit(sparse_vstack([X_train_trigram, X_val_trigram]), np.concatenate((y_train, y_val)))\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 86,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<div>\\n\",\n",
    "       \"<style scoped>\\n\",\n",
    "       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "       \"        vertical-align: middle;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe tbody tr th {\\n\",\n",
    "       \"        vertical-align: top;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"\\n\",\n",
    "       \"    .dataframe thead th {\\n\",\n",
    "       \"        text-align: right;\\n\",\n",
    "       \"    }\\n\",\n",
    "       \"</style>\\n\",\n",
    "       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
    "       \"      <th></th>\\n\",\n",
    "       \"      <th>index</th>\\n\",\n",
    "       \"      <th>mean_fit_time</th>\\n\",\n",
    "       \"      <th>std_fit_time</th>\\n\",\n",
    "       \"      <th>mean_score_time</th>\\n\",\n",
    "       \"      <th>std_score_time</th>\\n\",\n",
    "       \"      <th>param_alpha</th>\\n\",\n",
    "       \"      <th>param_n_iter_no_change</th>\\n\",\n",
    "       \"      <th>params</th>\\n\",\n",
    "       \"      <th>split0_test_accuracy</th>\\n\",\n",
    "       \"      <th>split1_test_accuracy</th>\\n\",\n",
    "       \"      <th>...</th>\\n\",\n",
    "       \"      <th>std_test_accuracy</th>\\n\",\n",
    "       \"      <th>rank_test_accuracy</th>\\n\",\n",
    "       \"      <th>split0_test_f1</th>\\n\",\n",
    "       \"      <th>split1_test_f1</th>\\n\",\n",
    "       \"      <th>split2_test_f1</th>\\n\",\n",
    "       \"      <th>split3_test_f1</th>\\n\",\n",
    "       \"      <th>split4_test_f1</th>\\n\",\n",
    "       \"      <th>mean_test_f1</th>\\n\",\n",
    "       \"      <th>std_test_f1</th>\\n\",\n",
    "       \"      <th>rank_test_f1</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>0</th>\\n\",\n",
    "       \"      <td>0</td>\\n\",\n",
    "       \"      <td>32.057936</td>\\n\",\n",
    "       \"      <td>4.823794</td>\\n\",\n",
    "       \"      <td>1.462032</td>\\n\",\n",
    "       \"      <td>0.469907</td>\\n\",\n",
    "       \"      <td>0.01</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>{'alpha': 0.01, 'n_iter_no_change': 5}</td>\\n\",\n",
    "       \"      <td>0.692152</td>\\n\",\n",
    "       \"      <td>0.691969</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.000602</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>0.352870</td>\\n\",\n",
    "       \"      <td>0.351276</td>\\n\",\n",
    "       \"      <td>0.352789</td>\\n\",\n",
    "       \"      <td>0.354524</td>\\n\",\n",
    "       \"      <td>0.356614</td>\\n\",\n",
    "       \"      <td>0.353614</td>\\n\",\n",
    "       \"      <td>0.001818</td>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>1</th>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>34.839753</td>\\n\",\n",
    "       \"      <td>5.847555</td>\\n\",\n",
    "       \"      <td>1.570614</td>\\n\",\n",
    "       \"      <td>0.270366</td>\\n\",\n",
    "       \"      <td>0.01</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>{'alpha': 0.01, 'n_iter_no_change': 10}</td>\\n\",\n",
    "       \"      <td>0.691190</td>\\n\",\n",
    "       \"      <td>0.691969</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.000497</td>\\n\",\n",
    "       \"      <td>18</td>\\n\",\n",
    "       \"      <td>0.347076</td>\\n\",\n",
    "       \"      <td>0.351426</td>\\n\",\n",
    "       \"      <td>0.348466</td>\\n\",\n",
    "       \"      <td>0.349686</td>\\n\",\n",
    "       \"      <td>0.349993</td>\\n\",\n",
    "       \"      <td>0.349329</td>\\n\",\n",
    "       \"      <td>0.001469</td>\\n\",\n",
    "       \"      <td>18</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>2</th>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>41.940447</td>\\n\",\n",
    "       \"      <td>10.329488</td>\\n\",\n",
    "       \"      <td>1.597674</td>\\n\",\n",
    "       \"      <td>0.275955</td>\\n\",\n",
    "       \"      <td>0.01</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'alpha': 0.01, 'n_iter_no_change': 15}</td>\\n\",\n",
    "       \"      <td>0.691122</td>\\n\",\n",
    "       \"      <td>0.691461</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.000457</td>\\n\",\n",
    "       \"      <td>19</td>\\n\",\n",
    "       \"      <td>0.346570</td>\\n\",\n",
    "       \"      <td>0.348641</td>\\n\",\n",
    "       \"      <td>0.347652</td>\\n\",\n",
    "       \"      <td>0.350843</td>\\n\",\n",
    "       \"      <td>0.350410</td>\\n\",\n",
    "       \"      <td>0.348823</td>\\n\",\n",
    "       \"      <td>0.001617</td>\\n\",\n",
    "       \"      <td>19</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>3</th>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>44.677039</td>\\n\",\n",
    "       \"      <td>6.685752</td>\\n\",\n",
    "       \"      <td>1.307405</td>\\n\",\n",
    "       \"      <td>0.393770</td>\\n\",\n",
    "       \"      <td>0.01</td>\\n\",\n",
    "       \"      <td>20</td>\\n\",\n",
    "       \"      <td>{'alpha': 0.01, 'n_iter_no_change': 20}</td>\\n\",\n",
    "       \"      <td>0.690916</td>\\n\",\n",
    "       \"      <td>0.690843</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.000539</td>\\n\",\n",
    "       \"      <td>20</td>\\n\",\n",
    "       \"      <td>0.345848</td>\\n\",\n",
    "       \"      <td>0.345646</td>\\n\",\n",
    "       \"      <td>0.346039</td>\\n\",\n",
    "       \"      <td>0.347637</td>\\n\",\n",
    "       \"      <td>0.349897</td>\\n\",\n",
    "       \"      <td>0.347013</td>\\n\",\n",
    "       \"      <td>0.001605</td>\\n\",\n",
    "       \"      <td>20</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>4</th>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>24.846832</td>\\n\",\n",
    "       \"      <td>6.631401</td>\\n\",\n",
    "       \"      <td>1.670751</td>\\n\",\n",
    "       \"      <td>0.193314</td>\\n\",\n",
    "       \"      <td>0.001</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>{'alpha': 0.001, 'n_iter_no_change': 5}</td>\\n\",\n",
    "       \"      <td>0.741415</td>\\n\",\n",
    "       \"      <td>0.742827</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.001528</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>0.546314</td>\\n\",\n",
    "       \"      <td>0.550779</td>\\n\",\n",
    "       \"      <td>0.547320</td>\\n\",\n",
    "       \"      <td>0.547302</td>\\n\",\n",
    "       \"      <td>0.555941</td>\\n\",\n",
    "       \"      <td>0.549531</td>\\n\",\n",
    "       \"      <td>0.003546</td>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>5</th>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>31.927570</td>\\n\",\n",
    "       \"      <td>2.169471</td>\\n\",\n",
    "       \"      <td>0.891322</td>\\n\",\n",
    "       \"      <td>0.097146</td>\\n\",\n",
    "       \"      <td>0.001</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>{'alpha': 0.001, 'n_iter_no_change': 10}</td>\\n\",\n",
    "       \"      <td>0.742473</td>\\n\",\n",
    "       \"      <td>0.743638</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.001614</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>0.552838</td>\\n\",\n",
    "       \"      <td>0.556443</td>\\n\",\n",
    "       \"      <td>0.547028</td>\\n\",\n",
    "       \"      <td>0.549382</td>\\n\",\n",
    "       \"      <td>0.549654</td>\\n\",\n",
    "       \"      <td>0.551069</td>\\n\",\n",
    "       \"      <td>0.003262</td>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>6</th>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>35.785039</td>\\n\",\n",
    "       \"      <td>6.948313</td>\\n\",\n",
    "       \"      <td>1.889833</td>\\n\",\n",
    "       \"      <td>0.513881</td>\\n\",\n",
    "       \"      <td>0.001</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'alpha': 0.001, 'n_iter_no_change': 15}</td>\\n\",\n",
    "       \"      <td>0.741072</td>\\n\",\n",
    "       \"      <td>0.743129</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.001499</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>0.545130</td>\\n\",\n",
    "       \"      <td>0.551458</td>\\n\",\n",
    "       \"      <td>0.546284</td>\\n\",\n",
    "       \"      <td>0.545874</td>\\n\",\n",
    "       \"      <td>0.550921</td>\\n\",\n",
    "       \"      <td>0.547933</td>\\n\",\n",
    "       \"      <td>0.002690</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>7</th>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>59.535236</td>\\n\",\n",
    "       \"      <td>2.297733</td>\\n\",\n",
    "       \"      <td>1.418594</td>\\n\",\n",
    "       \"      <td>0.290130</td>\\n\",\n",
    "       \"      <td>0.001</td>\\n\",\n",
    "       \"      <td>20</td>\\n\",\n",
    "       \"      <td>{'alpha': 0.001, 'n_iter_no_change': 20}</td>\\n\",\n",
    "       \"      <td>0.741236</td>\\n\",\n",
    "       \"      <td>0.741796</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.001460</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>0.546055</td>\\n\",\n",
    "       \"      <td>0.544507</td>\\n\",\n",
    "       \"      <td>0.539016</td>\\n\",\n",
    "       \"      <td>0.542346</td>\\n\",\n",
    "       \"      <td>0.547395</td>\\n\",\n",
    "       \"      <td>0.543864</td>\\n\",\n",
    "       \"      <td>0.002949</td>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>8</th>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>32.525184</td>\\n\",\n",
    "       \"      <td>8.522953</td>\\n\",\n",
    "       \"      <td>1.530440</td>\\n\",\n",
    "       \"      <td>0.418499</td>\\n\",\n",
    "       \"      <td>0.0001</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>{'alpha': 0.0001, 'n_iter_no_change': 5}</td>\\n\",\n",
    "       \"      <td>0.780660</td>\\n\",\n",
    "       \"      <td>0.782870</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.001323</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>0.652092</td>\\n\",\n",
    "       \"      <td>0.650675</td>\\n\",\n",
    "       \"      <td>0.650521</td>\\n\",\n",
    "       \"      <td>0.648484</td>\\n\",\n",
    "       \"      <td>0.657971</td>\\n\",\n",
    "       \"      <td>0.651949</td>\\n\",\n",
    "       \"      <td>0.003223</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>9</th>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>49.180857</td>\\n\",\n",
    "       \"      <td>11.419878</td>\\n\",\n",
    "       \"      <td>1.625803</td>\\n\",\n",
    "       \"      <td>0.334466</td>\\n\",\n",
    "       \"      <td>0.0001</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>{'alpha': 0.0001, 'n_iter_no_change': 10}</td>\\n\",\n",
    "       \"      <td>0.780578</td>\\n\",\n",
    "       \"      <td>0.782719</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.001481</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>0.650713</td>\\n\",\n",
    "       \"      <td>0.653565</td>\\n\",\n",
    "       \"      <td>0.643136</td>\\n\",\n",
    "       \"      <td>0.640263</td>\\n\",\n",
    "       \"      <td>0.642371</td>\\n\",\n",
    "       \"      <td>0.646010</td>\\n\",\n",
    "       \"      <td>0.005172</td>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>10</th>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>40.283555</td>\\n\",\n",
    "       \"      <td>10.689865</td>\\n\",\n",
    "       \"      <td>1.386696</td>\\n\",\n",
    "       \"      <td>0.343637</td>\\n\",\n",
    "       \"      <td>0.0001</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'alpha': 0.0001, 'n_iter_no_change': 15}</td>\\n\",\n",
    "       \"      <td>0.780908</td>\\n\",\n",
    "       \"      <td>0.782870</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.001463</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>0.653647</td>\\n\",\n",
    "       \"      <td>0.655294</td>\\n\",\n",
    "       \"      <td>0.651344</td>\\n\",\n",
    "       \"      <td>0.648414</td>\\n\",\n",
    "       \"      <td>0.656066</td>\\n\",\n",
    "       \"      <td>0.652953</td>\\n\",\n",
    "       \"      <td>0.002787</td>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>11</th>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>57.527124</td>\\n\",\n",
    "       \"      <td>5.102434</td>\\n\",\n",
    "       \"      <td>1.504030</td>\\n\",\n",
    "       \"      <td>0.103422</td>\\n\",\n",
    "       \"      <td>0.0001</td>\\n\",\n",
    "       \"      <td>20</td>\\n\",\n",
    "       \"      <td>{'alpha': 0.0001, 'n_iter_no_change': 20}</td>\\n\",\n",
    "       \"      <td>0.780688</td>\\n\",\n",
    "       \"      <td>0.782691</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.001637</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"      <td>0.650919</td>\\n\",\n",
    "       \"      <td>0.652103</td>\\n\",\n",
    "       \"      <td>0.647006</td>\\n\",\n",
    "       \"      <td>0.647918</td>\\n\",\n",
    "       \"      <td>0.656548</td>\\n\",\n",
    "       \"      <td>0.650899</td>\\n\",\n",
    "       \"      <td>0.003388</td>\\n\",\n",
    "       \"      <td>11</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>12</th>\\n\",\n",
    "       \"      <td>12</td>\\n\",\n",
    "       \"      <td>33.256662</td>\\n\",\n",
    "       \"      <td>3.574577</td>\\n\",\n",
    "       \"      <td>1.443488</td>\\n\",\n",
    "       \"      <td>0.112564</td>\\n\",\n",
    "       \"      <td>0.00001</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>{'alpha': 1e-05, 'n_iter_no_change': 5}</td>\\n\",\n",
    "       \"      <td>0.807332</td>\\n\",\n",
    "       \"      <td>0.808237</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.001453</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>0.698025</td>\\n\",\n",
    "       \"      <td>0.714616</td>\\n\",\n",
    "       \"      <td>0.709538</td>\\n\",\n",
    "       \"      <td>0.707157</td>\\n\",\n",
    "       \"      <td>0.707429</td>\\n\",\n",
    "       \"      <td>0.707353</td>\\n\",\n",
    "       \"      <td>0.005377</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>13</th>\\n\",\n",
    "       \"      <td>13</td>\\n\",\n",
    "       \"      <td>32.103819</td>\\n\",\n",
    "       \"      <td>3.748073</td>\\n\",\n",
    "       \"      <td>1.302405</td>\\n\",\n",
    "       \"      <td>0.426505</td>\\n\",\n",
    "       \"      <td>0.00001</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>{'alpha': 1e-05, 'n_iter_no_change': 10}</td>\\n\",\n",
    "       \"      <td>0.806453</td>\\n\",\n",
    "       \"      <td>0.807934</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.001237</td>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>0.713679</td>\\n\",\n",
    "       \"      <td>0.701722</td>\\n\",\n",
    "       \"      <td>0.710008</td>\\n\",\n",
    "       \"      <td>0.709509</td>\\n\",\n",
    "       \"      <td>0.719614</td>\\n\",\n",
    "       \"      <td>0.710906</td>\\n\",\n",
    "       \"      <td>0.005842</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>14</th>\\n\",\n",
    "       \"      <td>14</td>\\n\",\n",
    "       \"      <td>44.492694</td>\\n\",\n",
    "       \"      <td>0.965001</td>\\n\",\n",
    "       \"      <td>1.558569</td>\\n\",\n",
    "       \"      <td>0.328143</td>\\n\",\n",
    "       \"      <td>0.00001</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'alpha': 1e-05, 'n_iter_no_change': 15}</td>\\n\",\n",
    "       \"      <td>0.804680</td>\\n\",\n",
    "       \"      <td>0.808828</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.001937</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>0.713057</td>\\n\",\n",
    "       \"      <td>0.706113</td>\\n\",\n",
    "       \"      <td>0.711488</td>\\n\",\n",
    "       \"      <td>0.711608</td>\\n\",\n",
    "       \"      <td>0.715967</td>\\n\",\n",
    "       \"      <td>0.711647</td>\\n\",\n",
    "       \"      <td>0.003203</td>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>15</th>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>58.558907</td>\\n\",\n",
    "       \"      <td>10.112322</td>\\n\",\n",
    "       \"      <td>1.424766</td>\\n\",\n",
    "       \"      <td>0.203237</td>\\n\",\n",
    "       \"      <td>0.00001</td>\\n\",\n",
    "       \"      <td>20</td>\\n\",\n",
    "       \"      <td>{'alpha': 1e-05, 'n_iter_no_change': 20}</td>\\n\",\n",
    "       \"      <td>0.806742</td>\\n\",\n",
    "       \"      <td>0.807426</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.001874</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>0.712181</td>\\n\",\n",
    "       \"      <td>0.711004</td>\\n\",\n",
    "       \"      <td>0.715343</td>\\n\",\n",
    "       \"      <td>0.710696</td>\\n\",\n",
    "       \"      <td>0.715824</td>\\n\",\n",
    "       \"      <td>0.713009</td>\\n\",\n",
    "       \"      <td>0.002164</td>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>16</th>\\n\",\n",
    "       \"      <td>16</td>\\n\",\n",
    "       \"      <td>39.761328</td>\\n\",\n",
    "       \"      <td>0.877920</td>\\n\",\n",
    "       \"      <td>1.103284</td>\\n\",\n",
    "       \"      <td>0.397345</td>\\n\",\n",
    "       \"      <td>0.000001</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>{'alpha': 1e-06, 'n_iter_no_change': 5}</td>\\n\",\n",
    "       \"      <td>0.802949</td>\\n\",\n",
    "       \"      <td>0.803784</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.003224</td>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>0.706448</td>\\n\",\n",
    "       \"      <td>0.698482</td>\\n\",\n",
    "       \"      <td>0.704552</td>\\n\",\n",
    "       \"      <td>0.708678</td>\\n\",\n",
    "       \"      <td>0.718208</td>\\n\",\n",
    "       \"      <td>0.707274</td>\\n\",\n",
    "       \"      <td>0.006433</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>17</th>\\n\",\n",
    "       \"      <td>17</td>\\n\",\n",
    "       \"      <td>45.833273</td>\\n\",\n",
    "       \"      <td>3.359309</td>\\n\",\n",
    "       \"      <td>1.549358</td>\\n\",\n",
    "       \"      <td>0.397857</td>\\n\",\n",
    "       \"      <td>0.000001</td>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>{'alpha': 1e-06, 'n_iter_no_change': 10}</td>\\n\",\n",
    "       \"      <td>0.801602</td>\\n\",\n",
    "       \"      <td>0.802960</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.002758</td>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>0.708099</td>\\n\",\n",
    "       \"      <td>0.707074</td>\\n\",\n",
    "       \"      <td>0.710392</td>\\n\",\n",
    "       \"      <td>0.709978</td>\\n\",\n",
    "       \"      <td>0.715186</td>\\n\",\n",
    "       \"      <td>0.710146</td>\\n\",\n",
    "       \"      <td>0.002797</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>18</th>\\n\",\n",
    "       \"      <td>18</td>\\n\",\n",
    "       \"      <td>56.188770</td>\\n\",\n",
    "       \"      <td>2.537069</td>\\n\",\n",
    "       \"      <td>1.026216</td>\\n\",\n",
    "       \"      <td>0.285707</td>\\n\",\n",
    "       \"      <td>0.000001</td>\\n\",\n",
    "       \"      <td>15</td>\\n\",\n",
    "       \"      <td>{'alpha': 1e-06, 'n_iter_no_change': 15}</td>\\n\",\n",
    "       \"      <td>0.798497</td>\\n\",\n",
    "       \"      <td>0.802273</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.001822</td>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>0.714152</td>\\n\",\n",
    "       \"      <td>0.715007</td>\\n\",\n",
    "       \"      <td>0.711416</td>\\n\",\n",
    "       \"      <td>0.706668</td>\\n\",\n",
    "       \"      <td>0.719169</td>\\n\",\n",
    "       \"      <td>0.713283</td>\\n\",\n",
    "       \"      <td>0.004138</td>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <th>19</th>\\n\",\n",
    "       \"      <td>19</td>\\n\",\n",
    "       \"      <td>50.262399</td>\\n\",\n",
    "       \"      <td>7.179083</td>\\n\",\n",
    "       \"      <td>0.540624</td>\\n\",\n",
    "       \"      <td>0.348990</td>\\n\",\n",
    "       \"      <td>0.000001</td>\\n\",\n",
    "       \"      <td>20</td>\\n\",\n",
    "       \"      <td>{'alpha': 1e-06, 'n_iter_no_change': 20}</td>\\n\",\n",
    "       \"      <td>0.797260</td>\\n\",\n",
    "       \"      <td>0.799813</td>\\n\",\n",
    "       \"      <td>...</td>\\n\",\n",
    "       \"      <td>0.002437</td>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>0.712611</td>\\n\",\n",
    "       \"      <td>0.715613</td>\\n\",\n",
    "       \"      <td>0.713257</td>\\n\",\n",
    "       \"      <td>0.711372</td>\\n\",\n",
    "       \"      <td>0.716722</td>\\n\",\n",
    "       \"      <td>0.713915</td>\\n\",\n",
    "       \"      <td>0.001968</td>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table>\\n\",\n",
    "       \"<p>20 rows × 24 columns</p>\\n\",\n",
    "       \"</div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"    index  mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\\\\n\",\n",
    "       \"0       0      32.057936      4.823794         1.462032        0.469907   \\n\",\n",
    "       \"1       1      34.839753      5.847555         1.570614        0.270366   \\n\",\n",
    "       \"2       2      41.940447     10.329488         1.597674        0.275955   \\n\",\n",
    "       \"3       3      44.677039      6.685752         1.307405        0.393770   \\n\",\n",
    "       \"4       4      24.846832      6.631401         1.670751        0.193314   \\n\",\n",
    "       \"5       5      31.927570      2.169471         0.891322        0.097146   \\n\",\n",
    "       \"6       6      35.785039      6.948313         1.889833        0.513881   \\n\",\n",
    "       \"7       7      59.535236      2.297733         1.418594        0.290130   \\n\",\n",
    "       \"8       8      32.525184      8.522953         1.530440        0.418499   \\n\",\n",
    "       \"9       9      49.180857     11.419878         1.625803        0.334466   \\n\",\n",
    "       \"10     10      40.283555     10.689865         1.386696        0.343637   \\n\",\n",
    "       \"11     11      57.527124      5.102434         1.504030        0.103422   \\n\",\n",
    "       \"12     12      33.256662      3.574577         1.443488        0.112564   \\n\",\n",
    "       \"13     13      32.103819      3.748073         1.302405        0.426505   \\n\",\n",
    "       \"14     14      44.492694      0.965001         1.558569        0.328143   \\n\",\n",
    "       \"15     15      58.558907     10.112322         1.424766        0.203237   \\n\",\n",
    "       \"16     16      39.761328      0.877920         1.103284        0.397345   \\n\",\n",
    "       \"17     17      45.833273      3.359309         1.549358        0.397857   \\n\",\n",
    "       \"18     18      56.188770      2.537069         1.026216        0.285707   \\n\",\n",
    "       \"19     19      50.262399      7.179083         0.540624        0.348990   \\n\",\n",
    "       \"\\n\",\n",
    "       \"   param_alpha param_n_iter_no_change  \\\\\\n\",\n",
    "       \"0         0.01                      5   \\n\",\n",
    "       \"1         0.01                     10   \\n\",\n",
    "       \"2         0.01                     15   \\n\",\n",
    "       \"3         0.01                     20   \\n\",\n",
    "       \"4        0.001                      5   \\n\",\n",
    "       \"5        0.001                     10   \\n\",\n",
    "       \"6        0.001                     15   \\n\",\n",
    "       \"7        0.001                     20   \\n\",\n",
    "       \"8       0.0001                      5   \\n\",\n",
    "       \"9       0.0001                     10   \\n\",\n",
    "       \"10      0.0001                     15   \\n\",\n",
    "       \"11      0.0001                     20   \\n\",\n",
    "       \"12     0.00001                      5   \\n\",\n",
    "       \"13     0.00001                     10   \\n\",\n",
    "       \"14     0.00001                     15   \\n\",\n",
    "       \"15     0.00001                     20   \\n\",\n",
    "       \"16    0.000001                      5   \\n\",\n",
    "       \"17    0.000001                     10   \\n\",\n",
    "       \"18    0.000001                     15   \\n\",\n",
    "       \"19    0.000001                     20   \\n\",\n",
    "       \"\\n\",\n",
    "       \"                                       params  split0_test_accuracy  \\\\\\n\",\n",
    "       \"0      {'alpha': 0.01, 'n_iter_no_change': 5}              0.692152   \\n\",\n",
    "       \"1     {'alpha': 0.01, 'n_iter_no_change': 10}              0.691190   \\n\",\n",
    "       \"2     {'alpha': 0.01, 'n_iter_no_change': 15}              0.691122   \\n\",\n",
    "       \"3     {'alpha': 0.01, 'n_iter_no_change': 20}              0.690916   \\n\",\n",
    "       \"4     {'alpha': 0.001, 'n_iter_no_change': 5}              0.741415   \\n\",\n",
    "       \"5    {'alpha': 0.001, 'n_iter_no_change': 10}              0.742473   \\n\",\n",
    "       \"6    {'alpha': 0.001, 'n_iter_no_change': 15}              0.741072   \\n\",\n",
    "       \"7    {'alpha': 0.001, 'n_iter_no_change': 20}              0.741236   \\n\",\n",
    "       \"8    {'alpha': 0.0001, 'n_iter_no_change': 5}              0.780660   \\n\",\n",
    "       \"9   {'alpha': 0.0001, 'n_iter_no_change': 10}              0.780578   \\n\",\n",
    "       \"10  {'alpha': 0.0001, 'n_iter_no_change': 15}              0.780908   \\n\",\n",
    "       \"11  {'alpha': 0.0001, 'n_iter_no_change': 20}              0.780688   \\n\",\n",
    "       \"12    {'alpha': 1e-05, 'n_iter_no_change': 5}              0.807332   \\n\",\n",
    "       \"13   {'alpha': 1e-05, 'n_iter_no_change': 10}              0.806453   \\n\",\n",
    "       \"14   {'alpha': 1e-05, 'n_iter_no_change': 15}              0.804680   \\n\",\n",
    "       \"15   {'alpha': 1e-05, 'n_iter_no_change': 20}              0.806742   \\n\",\n",
    "       \"16    {'alpha': 1e-06, 'n_iter_no_change': 5}              0.802949   \\n\",\n",
    "       \"17   {'alpha': 1e-06, 'n_iter_no_change': 10}              0.801602   \\n\",\n",
    "       \"18   {'alpha': 1e-06, 'n_iter_no_change': 15}              0.798497   \\n\",\n",
    "       \"19   {'alpha': 1e-06, 'n_iter_no_change': 20}              0.797260   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    split1_test_accuracy  ...  std_test_accuracy  rank_test_accuracy  \\\\\\n\",\n",
    "       \"0               0.691969  ...           0.000602                  17   \\n\",\n",
    "       \"1               0.691969  ...           0.000497                  18   \\n\",\n",
    "       \"2               0.691461  ...           0.000457                  19   \\n\",\n",
    "       \"3               0.690843  ...           0.000539                  20   \\n\",\n",
    "       \"4               0.742827  ...           0.001528                  14   \\n\",\n",
    "       \"5               0.743638  ...           0.001614                  13   \\n\",\n",
    "       \"6               0.743129  ...           0.001499                  15   \\n\",\n",
    "       \"7               0.741796  ...           0.001460                  16   \\n\",\n",
    "       \"8               0.782870  ...           0.001323                   9   \\n\",\n",
    "       \"9               0.782719  ...           0.001481                  12   \\n\",\n",
    "       \"10              0.782870  ...           0.001463                  10   \\n\",\n",
    "       \"11              0.782691  ...           0.001637                  11   \\n\",\n",
    "       \"12              0.808237  ...           0.001453                   1   \\n\",\n",
    "       \"13              0.807934  ...           0.001237                   4   \\n\",\n",
    "       \"14              0.808828  ...           0.001937                   2   \\n\",\n",
    "       \"15              0.807426  ...           0.001874                   3   \\n\",\n",
    "       \"16              0.803784  ...           0.003224                   7   \\n\",\n",
    "       \"17              0.802960  ...           0.002758                   5   \\n\",\n",
    "       \"18              0.802273  ...           0.001822                   6   \\n\",\n",
    "       \"19              0.799813  ...           0.002437                   8   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    split0_test_f1  split1_test_f1  split2_test_f1  split3_test_f1  \\\\\\n\",\n",
    "       \"0         0.352870        0.351276        0.352789        0.354524   \\n\",\n",
    "       \"1         0.347076        0.351426        0.348466        0.349686   \\n\",\n",
    "       \"2         0.346570        0.348641        0.347652        0.350843   \\n\",\n",
    "       \"3         0.345848        0.345646        0.346039        0.347637   \\n\",\n",
    "       \"4         0.546314        0.550779        0.547320        0.547302   \\n\",\n",
    "       \"5         0.552838        0.556443        0.547028        0.549382   \\n\",\n",
    "       \"6         0.545130        0.551458        0.546284        0.545874   \\n\",\n",
    "       \"7         0.546055        0.544507        0.539016        0.542346   \\n\",\n",
    "       \"8         0.652092        0.650675        0.650521        0.648484   \\n\",\n",
    "       \"9         0.650713        0.653565        0.643136        0.640263   \\n\",\n",
    "       \"10        0.653647        0.655294        0.651344        0.648414   \\n\",\n",
    "       \"11        0.650919        0.652103        0.647006        0.647918   \\n\",\n",
    "       \"12        0.698025        0.714616        0.709538        0.707157   \\n\",\n",
    "       \"13        0.713679        0.701722        0.710008        0.709509   \\n\",\n",
    "       \"14        0.713057        0.706113        0.711488        0.711608   \\n\",\n",
    "       \"15        0.712181        0.711004        0.715343        0.710696   \\n\",\n",
    "       \"16        0.706448        0.698482        0.704552        0.708678   \\n\",\n",
    "       \"17        0.708099        0.707074        0.710392        0.709978   \\n\",\n",
    "       \"18        0.714152        0.715007        0.711416        0.706668   \\n\",\n",
    "       \"19        0.712611        0.715613        0.713257        0.711372   \\n\",\n",
    "       \"\\n\",\n",
    "       \"    split4_test_f1  mean_test_f1  std_test_f1  rank_test_f1  \\n\",\n",
    "       \"0         0.356614      0.353614     0.001818            17  \\n\",\n",
    "       \"1         0.349993      0.349329     0.001469            18  \\n\",\n",
    "       \"2         0.350410      0.348823     0.001617            19  \\n\",\n",
    "       \"3         0.349897      0.347013     0.001605            20  \\n\",\n",
    "       \"4         0.555941      0.549531     0.003546            14  \\n\",\n",
    "       \"5         0.549654      0.551069     0.003262            13  \\n\",\n",
    "       \"6         0.550921      0.547933     0.002690            15  \\n\",\n",
    "       \"7         0.547395      0.543864     0.002949            16  \\n\",\n",
    "       \"8         0.657971      0.651949     0.003223            10  \\n\",\n",
    "       \"9         0.642371      0.646010     0.005172            12  \\n\",\n",
    "       \"10        0.656066      0.652953     0.002787             9  \\n\",\n",
    "       \"11        0.656548      0.650899     0.003388            11  \\n\",\n",
    "       \"12        0.707429      0.707353     0.005377             7  \\n\",\n",
    "       \"13        0.719614      0.710906     0.005842             5  \\n\",\n",
    "       \"14        0.715967      0.711647     0.003203             4  \\n\",\n",
    "       \"15        0.715824      0.713009     0.002164             3  \\n\",\n",
    "       \"16        0.718208      0.707274     0.006433             8  \\n\",\n",
    "       \"17        0.715186      0.710146     0.002797             6  \\n\",\n",
    "       \"18        0.719169      0.713283     0.004138             2  \\n\",\n",
    "       \"19        0.716722      0.713915     0.001968             1  \\n\",\n",
    "       \"\\n\",\n",
    "       \"[20 rows x 24 columns]\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 86,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"gridSearchCVResults = pd.DataFrame.from_dict(grid_search.cv_results_).reset_index()\\n\",\n",
    "    \"gridSearchCVResults\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"<u>Best Parameters</u>\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 87,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Best Parameters:  {'alpha': 1e-06, 'n_iter_no_change': 20}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"print(\\\"Best Parameters: \\\", grid_search.best_params_)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 92,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"trigram Tuned Logistic Regression Accuracy:  80.31116277919315\\n\",\n",
    "      \"trigram Tuned Logistic Regression F1 Score:  72.37454015409178\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"bestAlpha = grid_search.best_params_['alpha']\\n\",\n",
    "    \"bestNIterNoChange = grid_search.best_params_['n_iter_no_change']\\n\",\n",
    "    \"trigramTunedLogisticRegressor = SGDClassifier(\\n\",\n",
    "    \"                            loss='log_loss',\\n\",\n",
    "    \"                            alpha=bestAlpha,\\n\",\n",
    "    \"                            penalty='l2',\\n\",\n",
    "    \"                            max_iter=1000,\\n\",\n",
    "    \"                            learning_rate='optimal',\\n\",\n",
    "    \"                            n_iter_no_change=bestNIterNoChange,\\n\",\n",
    "    \"                            n_jobs=-1,\\n\",\n",
    "    \"                            random_state=42)\\n\",\n",
    "    \"trigramTunedLogisticRegressor.fit(X_train_trigram, y_train)\\n\",\n",
    "    \"y_pred_trigram_tuned_logistic = trigramTunedLogisticRegressor.predict(X_test_trigram)\\n\",\n",
    "    \"print(\\\"trigram Tuned Logistic Regression Accuracy: \\\", 100*accuracy_score(y_test, y_pred_trigram_tuned_logistic))\\n\",\n",
    "    \"print(\\\"trigram Tuned Logistic Regression F1 Score: \\\", 100*f1_score(y_test, y_pred_trigram_tuned_logistic))\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## SVM\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"According to paper, the default parameters to be used for SVM are $C=1.0$ and $kernel=linear$ unless specified otherwise.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"##### Unigram Linear SVM Model \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 8,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Unigram Linear SVM Accuracy:  0.7339038808775878\\n\",\n",
    "      \"Unigram Linear SVM F1 Score:  0.6413282656531306\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# X_train_unigram = load_npz(UNIGRAM_PATH + \\\"train.npz\\\")\\n\",\n",
    "    \"# X_test_unigram = load_npz(UNIGRAM_PATH + \\\"test.npz\\\")\\n\",\n",
    "    \"unigramLinearSVM = LinearSVC(C=1.0, max_iter=10000, random_state=42)\\n\",\n",
    "    \"unigramLinearSVM.fit(X_train_unigram, y_train)\\n\",\n",
    "    \"y_pred_unigram_linear_svm = unigramLinearSVM.predict(X_test_unigram)\\n\",\n",
    "    \"print(\\\"Unigram Linear SVM Accuracy: \\\", accuracy_score(y_test, y_pred_unigram_linear_svm))\\n\",\n",
    "    \"print(\\\"Unigram Linear SVM F1 Score: \\\", f1_score(y_test, y_pred_unigram_linear_svm))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"##### Bigram Linear SVM Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 8,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Bigram Linear SVM Accuracy:  0.7765465383759182\\n\",\n",
    "      \"Bigram Linear SVM F1 Score:  0.6993877279382404\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# X_train_bigram = load_npz(BIGRAM_PATH + \\\"train.npz\\\")\\n\",\n",
    "    \"# X_test_bigram = load_npz(BIGRAM_PATH + \\\"test.npz\\\")\\n\",\n",
    "    \"bigramLinearSVM = LinearSVC(C=1.0, max_iter=10000, random_state=42)\\n\",\n",
    "    \"bigramLinearSVM.fit(X_train_bigram, y_train)\\n\",\n",
    "    \"y_pred_bigram_linear_svm = bigramLinearSVM.predict(X_test_bigram)\\n\",\n",
    "    \"print(\\\"Bigram Linear SVM Accuracy: \\\", accuracy_score(y_test, y_pred_bigram_linear_svm))\\n\",\n",
    "    \"print(\\\"Bigram Linear SVM F1 Score: \\\", f1_score(y_test, y_pred_bigram_linear_svm))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"##### Trigram Linear SVM Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 8,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Trigram Linear SVM Accuracy:  0.7926241064582354\\n\",\n",
    "      \"Trigram Linear SVM F1 Score:  0.7131910235358511\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# X_train_trigram = load_npz(TRIGRAM_PATH + \\\"train.npz\\\")\\n\",\n",
    "    \"# X_test_trigram = load_npz(TRIGRAM_PATH + \\\"test.npz\\\")\\n\",\n",
    "    \"trigramLinearSVM = LinearSVC(C=1.0, max_iter=10000, random_state=42)\\n\",\n",
    "    \"trigramLinearSVM.fit(X_train_trigram, y_train)\\n\",\n",
    "    \"y_pred_trigram_linear_svm = trigramLinearSVM.predict(X_test_trigram)\\n\",\n",
    "    \"print(\\\"Trigram Linear SVM Accuracy: \\\", accuracy_score(y_test, y_pred_trigram_linear_svm))\\n\",\n",
    "    \"print(\\\"Trigram Linear SVM F1 Score: \\\", f1_score(y_test, y_pred_trigram_linear_svm))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Parameter tuning for Trigram SVM Model with different kernels\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 10,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# X_train_trigram = load_npz(TRIGRAM_PATH + \\\"train.npz\\\")\\n\",\n",
    "    \"# X_test_trigram = load_npz(TRIGRAM_PATH + \\\"test.npz\\\")\\n\",\n",
    "    \"# X_val_trigram = load_npz(TRIGRAM_PATH + \\\"val.npz\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 12,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Fitting 2 folds for each of 14 candidates, totalling 28 fits\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"parameters = dict({\\n\",\n",
    "    \"                    'C':[0.001, 0.005, 0.1, 0.5, 1.0, 10, 50], \\n\",\n",
    "    \"                    'kernel':['linear', 'rbf']\\n\",\n",
    "    \"                })\\n\",\n",
    "    \"trigramSVM = SVC(max_iter=-1, random_state=42, gamma='auto')\\n\",\n",
    "    \"cv_stratified_splitter = StratifiedKFold(n_splits=2)\\n\",\n",
    "    \"grid_search = GridSearchCV(trigramSVM, \\n\",\n",
    "    \"                            parameters, \\n\",\n",
    "    \"                            cv=cv_stratified_splitter, \\n\",\n",
    "    \"                            scoring=['accuracy', 'f1'], \\n\",\n",
    "    \"                            n_jobs=-1,\\n\",\n",
    "    \"                            refit='accuracy',\\n\",\n",
    "    \"                            verbose=1)\\n\",\n",
    "    \"grid_search.fit(sparse_vstack([X_train_trigram, X_val_trigram]), np.concatenate((y_train, y_val)))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 14,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Best Parameters:  {'C': 0.1, 'kernel': 'linear'}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"print(\\\"Best Parameters: \\\", grid_search.best_params_)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 15,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Trigram Tuned SVM Accuracy:  0.8011445843330283\\n\",\n",
    "      \"Trigram Tuned SVM F1 Score:  0.712977065767285\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"bestC=grid_search.best_params_['C']\\n\",\n",
    "    \"bestKernel=grid_search.best_params_['kernel']\\n\",\n",
    "    \"trigramTunedSVM = SVC(C=bestC, kernel=bestKernel, max_iter=-1, random_state=42, gamma='auto')\\n\",\n",
    "    \"trigramTunedSVM.fit(X_train_trigram, y_train)\\n\",\n",
    "    \"y_pred_trigram_tuned_svm = trigramTunedSVM.predict(X_test_trigram)\\n\",\n",
    "    \"print(\\\"Trigram Tuned SVM Accuracy: \\\", accuracy_score(y_test, y_pred_trigram_tuned_svm))\\n\",\n",
    "    \"print(\\\"Trigram Tuned SVM F1 Score: \\\", f1_score(y_test, y_pred_trigram_tuned_svm))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Sentence embeddings as feature vectors\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"50-dimensional word vectors are obtained using GloVe vectors (GloVe.6B.50d.txt). The sentence embeddings are obtained by simply summing the word embeddings in a sentence. The sentence embeddings are then used as feature vectors for classification in the following two ways:\\n\",\n",
    "    \"- Plain sentence embeddings\\n\",\n",
    "    \"- Distance measure between vectors\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 85,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"GLOVE_PATH = INPUT + 'glove.6B.50d.txt'\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 79,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def preprocessAndTokenizeForGlove(text: str) -> list[str]:\\n\",\n",
    "    \"    text = re.sub(r'[^\\\\x00-\\\\x7F]+',' ', text.lower())\\n\",\n",
    "    \"    text = text.translate(str.maketrans('', '', string.punctuation))\\n\",\n",
    "    \"    tokens = nltk.word_tokenize(text)\\n\",\n",
    "    \"    tokens = [stemmer.stem(w) for w in tokens if stemmer.stem(w) not in stopwords]\\n\",\n",
    "    \"    return tokens\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 83,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"try:\\n\",\n",
    "    \"    X_train_q1_tokenized = np.array([preprocessAndTokenizeForGlove(ques) for ques in X_train_q1], dtype=object)\\n\",\n",
    "    \"    X_train_q2_tokenized = np.array([preprocessAndTokenizeForGlove(ques) for ques in X_train_q2], dtype=object)\\n\",\n",
    "    \"    X_test_q1_tokenized = np.array([preprocessAndTokenizeForGlove(ques) for ques in X_test_q1], dtype=object)\\n\",\n",
    "    \"    X_test_q2_tokenized = np.array([preprocessAndTokenizeForGlove(ques) for ques in X_test_q2], dtype=object)\\n\",\n",
    "    \"    X_val_q1_tokenized = np.array([preprocessAndTokenizeForGlove(ques) for ques in X_val_q1], dtype=object)\\n\",\n",
    "    \"    X_val_q2_tokenized = np.array([preprocessAndTokenizeForGlove(ques) for ques in X_val_q2], dtype=object)\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(e)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 86,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load GloVe Word Embeddings\\n\",\n",
    "    \"GloVe_embeddings = {}\\n\",\n",
    "    \"with open(GLOVE_PATH, 'r') as f:\\n\",\n",
    "    \"    for line in f:\\n\",\n",
    "    \"        values = line.split()\\n\",\n",
    "    \"        word = values[0]\\n\",\n",
    "    \"        vector = np.asarray(values[1:], \\\"float32\\\")\\n\",\n",
    "    \"        GloVe_embeddings[word] = vector\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Creating sentence embeddings for each question\\n\",\n",
    "    \"\\n\",\n",
    "    \"According to the paper, the sentence embeddings are obtained by simply summing the embeddings of all the tokens in a sentence.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 181,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"X_train_q1_embeddings = np.array([np.sum([GloVe_embeddings[w] for w in ques if w in GloVe_embeddings] + [np.zeros((50,))], axis=0) for ques in X_train_q1_tokenized])\\n\",\n",
    "    \"X_train_q2_embeddings = np.array([np.sum([GloVe_embeddings[w] for w in ques if w in GloVe_embeddings] + [np.zeros((50,))], axis=0) for ques in X_train_q2_tokenized])\\n\",\n",
    "    \"X_test_q1_embeddings = np.array([np.sum([GloVe_embeddings[w] for w in ques if w in GloVe_embeddings] + [np.zeros((50,))], axis=0) for ques in X_test_q1_tokenized])\\n\",\n",
    "    \"X_test_q2_embeddings = np.array([np.sum([GloVe_embeddings[w] for w in ques if w in GloVe_embeddings] + [np.zeros((50,))], axis=0) for ques in X_test_q2_tokenized])\\n\",\n",
    "    \"X_val_q1_embeddings = np.array([np.sum([GloVe_embeddings[w] for w in ques if w in GloVe_embeddings] + [np.zeros((50,))], axis=0) for ques in X_val_q1_tokenized])\\n\",\n",
    "    \"X_val_q2_embeddings = np.array([np.sum([GloVe_embeddings[w] for w in ques if w in GloVe_embeddings] + [np.zeros((50,))], axis=0) for ques in X_val_q2_tokenized])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"QUESTION_EMBEDDINGS = './question_embeddings/'\\n\",\n",
    "    \"os.makedirs(QUESTION_EMBEDDINGS, exist_ok=True)\\n\",\n",
    "    \"TRAIN_Q1_EMBEDDINGS = QUESTION_EMBEDDINGS + 'train_q1_embeddings.npz'\\n\",\n",
    "    \"TRAIN_Q2_EMBEDDINGS = QUESTION_EMBEDDINGS + 'train_q2_embeddings.npz'\\n\",\n",
    "    \"TEST_Q1_EMBEDDINGS = QUESTION_EMBEDDINGS + 'test_q1_embeddings.npz'\\n\",\n",
    "    \"TEST_Q2_EMBEDDINGS = QUESTION_EMBEDDINGS + 'test_q2_embeddings.npz'\\n\",\n",
    "    \"VAL_Q1_EMBEDDINGS = QUESTION_EMBEDDINGS + 'val_q1_embeddings.npz'\\n\",\n",
    "    \"VAL_Q2_EMBEDDINGS = QUESTION_EMBEDDINGS + 'val_q2_embeddings.npz'\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 183,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"np.savez_compressed(TRAIN_Q1_EMBEDDINGS, X_train_q1_embeddings)\\n\",\n",
    "    \"np.savez_compressed(TRAIN_Q2_EMBEDDINGS, X_train_q2_embeddings)\\n\",\n",
    "    \"np.savez_compressed(TEST_Q1_EMBEDDINGS, X_test_q1_embeddings)\\n\",\n",
    "    \"np.savez_compressed(TEST_Q2_EMBEDDINGS, X_test_q2_embeddings)\\n\",\n",
    "    \"np.savez_compressed(VAL_Q1_EMBEDDINGS, X_val_q1_embeddings)\\n\",\n",
    "    \"np.savez_compressed(VAL_Q2_EMBEDDINGS, X_val_q2_embeddings)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"X_train_q1_embeddings = np.load(TRAIN_Q1_EMBEDDINGS)['arr_0']\\n\",\n",
    "    \"X_train_q2_embeddings = np.load(TRAIN_Q2_EMBEDDINGS)['arr_0']\\n\",\n",
    "    \"X_test_q1_embeddings = np.load(TEST_Q1_EMBEDDINGS)['arr_0']\\n\",\n",
    "    \"X_test_q2_embeddings = np.load(TEST_Q2_EMBEDDINGS)['arr_0']\\n\",\n",
    "    \"X_val_q1_embeddings = np.load(VAL_Q1_EMBEDDINGS)['arr_0']\\n\",\n",
    "    \"X_val_q2_embeddings = np.load(VAL_Q2_EMBEDDINGS)['arr_0']\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 1. Plain sentence embeddings\\n\",\n",
    "    \"\\n\",\n",
    "    \"$100$-dimensional feature vector = $50$-dimensional question $1$ sentence embedding + $50$-dimensional question $2$ sentence embedding\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 188,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"X_train_plain_embeddings = np.hstack((X_train_q1_embeddings, X_train_q2_embeddings))\\n\",\n",
    "    \"X_test_plain_embeddings = np.hstack((X_test_q1_embeddings, X_test_q2_embeddings))\\n\",\n",
    "    \"X_val_plain_embeddings = np.hstack((X_val_q1_embeddings, X_val_q2_embeddings))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 191,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Accuracy: 0.7738499097182715\\n\",\n",
    "      \"F1 Score: 0.6989414766371224\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"SVMmodel = SVC(kernel='rbf', C=1.0, random_state=42, max_iter=-1, gamma='auto')\\n\",\n",
    "    \"SVMmodel.fit(X_train_plain_embeddings, y_train)\\n\",\n",
    "    \"y_pred = SVMmodel.predict(X_test_plain_embeddings)\\n\",\n",
    "    \"print(\\\"Accuracy:\\\", accuracy_score(y_test, y_pred))\\n\",\n",
    "    \"print(\\\"F1 Score:\\\", f1_score(y_test, y_pred))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 193,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"/home/nemesis/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:301: ConvergenceWarning: Solver terminated early (max_iter=3500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\\n\",\n",
    "      \"  warnings.warn(\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Accuracy: 0.63856125306092164\\n\",\n",
    "      \"F1 Score: 0.6193053333333333\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"SVMmodel = SVC(kernel='linear', C=1.0, random_state=42, max_iter=-1, gamma='auto')\\n\",\n",
    "    \"SVMmodel.fit(X_train_plain_embeddings, y_train)\\n\",\n",
    "    \"y_pred = SVMmodel.predict(X_test_plain_embeddings)\\n\",\n",
    "    \"print(\\\"Accuracy:\\\", accuracy_score(y_test, y_pred))\\n\",\n",
    "    \"print(\\\"F1 Score:\\\", f1_score(y_test, y_pred))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 2. Distance measure between vectors\\n\",\n",
    "    \"Feature vector is obtained by taking various distance measures between the sentence embeddings of the two questions:\\n\",\n",
    "    \"- Bray Curtis distance:\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"d_{Bray Curtis} = \\\\frac{1}{2} \\\\sum_{i=1}^{50} \\\\frac{|q_1[i] - q_2[i]|}{|q_1[i] + q_2[i]|}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"- Canberra distance:\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"d_{Canberra} = \\\\sum_{i=1}^{50} \\\\frac{|q_1[i] - q_2[i]|}{|q_1[i]| + |q_2[i]|}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"- Chebyshev distance:\\n\",\n",
    "    \"$$d_{Chebyshev} = max(|q_1[i] - q_2[i]|)$$\\n\",\n",
    "    \"- City block distance:\\n\",\n",
    "    \"$$d_{City block} = \\\\sum_{i=1}^{50} |q_1[i] - q_2[i]|$$\\n\",\n",
    "    \"- Correlation distance:\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"\\\\begin{aligned}\\n\",\n",
    "    \"d_{Correlation} = 1 - \\\\frac{\\\\sum_{i=1}^{50} (q_1[i] - \\\\bar{q_1})(q_2[i] - \\\\bar{q_2})}{\\\\sqrt{\\\\sum_{i=1}^{50} (q_1[i] - \\\\bar{q_1})^2} \\\\sqrt{\\\\sum_{i=1}^{50} (q_2[i] - \\\\bar{q_2})^2}}\\n\",\n",
    "    \"\\\\end{aligned}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"- Cosine distance: \\n\",\n",
    "    \"$$\\n\",\n",
    "    \"d_{Cosine} = 1 - \\\\frac{\\\\sum_{i=1}^{50} q_1[i]q_2[i]}{\\\\sqrt{\\\\sum_{i=1}^{50} q_1[i]^2} \\\\sqrt{\\\\sum_{i=1}^{50} q_2[i]^2}}\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"- Euclidean distance:\\n\",\n",
    "    \"$$d_{Euclidean} = \\\\sqrt{\\\\sum_{i=1}^{50} (q_1[i] - q_2[i])^2}$$\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"/home/jeopardy/.local/lib/python3.10/site-packages/scipy/spatial/distance.py:1162: RuntimeWarning: invalid value encountered in double_scalars\\n\",\n",
    "      \"  return l1_diff.sum() / l1_sum.sum()\\n\",\n",
    "      \"/home/jeopardy/.local/lib/python3.10/site-packages/scipy/spatial/distance.py:630: RuntimeWarning: invalid value encountered in double_scalars\\n\",\n",
    "      \"  dist = 1.0 - uv / np.sqrt(uu * vv)\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import scipy.spatial.distance as scipyDistance\\n\",\n",
    "    \"\\n\",\n",
    "    \"def distances(q1, q2):\\n\",\n",
    "    \"    distanceFeatureVector = np.array([\\n\",\n",
    "    \"        scipyDistance.braycurtis(q1, q2),\\n\",\n",
    "    \"        scipyDistance.canberra(q1, q2),\\n\",\n",
    "    \"        scipyDistance.chebyshev(q1, q2),\\n\",\n",
    "    \"        scipyDistance.cityblock(q1, q2),\\n\",\n",
    "    \"        scipyDistance.correlation(q1, q2),\\n\",\n",
    "    \"        scipyDistance.cosine(q1, q2),\\n\",\n",
    "    \"        scipyDistance.euclidean(q1, q2)\\n\",\n",
    "    \"    ])\\n\",\n",
    "    \"    distanceFeatureVector = np.nan_to_num(distanceFeatureVector)\\n\",\n",
    "    \"    return distanceFeatureVector\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"X_train_distances = np.array([distances(q1, q2) for q1, q2 in zip(X_train_q1_embeddings, X_train_q2_embeddings)])\\n\",\n",
    "    \"X_test_distances = np.array([distances(q1, q2) for q1, q2 in zip(X_test_q1_embeddings, X_test_q2_embeddings)])\\n\",\n",
    "    \"X_val_distances = np.array([distances(q1, q2) for q1, q2 in zip(X_val_q1_embeddings, X_val_q2_embeddings)])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Accuracy: 0.683458686969944\\n\",\n",
    "      \"F1 Score: 0.679439494994484\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"distanceSVM = SVC(kernel='rbf', C=1.0, random_state=42, max_iter=-1, gamma='auto')\\n\",\n",
    "    \"distanceSVM.fit(X_train_distances, y_train)\\n\",\n",
    "    \"y_pred = distanceSVM.predict(X_test_distances)\\n\",\n",
    "    \"print(\\\"Accuracy:\\\", accuracy_score(y_test, y_pred))\\n\",\n",
    "    \"print(\\\"F1 Score:\\\", f1_score(y_test, y_pred))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 8,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Accuracy: 0.639099378488921\\n\",\n",
    "      \"F1 Score: 0.625289938484038\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"distanceSVM = SVC(kernel='linear', C=1.0, random_state=42, max_iter=-1, gamma='auto')\\n\",\n",
    "    \"distanceSVM.fit(X_train_distances, y_train)\\n\",\n",
    "    \"y_pred = distanceSVM.predict(X_test_distances)\\n\",\n",
    "    \"print(\\\"Accuracy:\\\", accuracy_score(y_test, y_pred))\\n\",\n",
    "    \"print(\\\"F1 Score:\\\", f1_score(y_test, y_pred))\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3.10.8 64-bit\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.4\"\n",
    "  },\n",
    "  \"orig_nbformat\": 4,\n",
    "  \"vscode\": {\n",
    "   \"interpreter\": {\n",
    "    \"hash\": \"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a\"\n",
    "   }\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
